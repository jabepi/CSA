I20241027 09:03:35.932972 10182 caffe.cpp:204] Using GPUs 0
I20241027 09:03:36.159318 10182 caffe.cpp:209] GPU 0: NVIDIA GeForce RTX 4090
I20241027 09:03:37.253279 10182 solver.cpp:45] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 10000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "examples/mnist/lenet_sigmoid"
solver_mode: GPU
device_id: 0
net: "examples/mnist/lenet_train_test_sigmoid.prototxt"
train_state {
  level: 0
  stage: ""
}
I20241027 09:03:37.270884 10182 solver.cpp:102] Creating training net from net file: examples/mnist/lenet_train_test_sigmoid.prototxt
I20241027 09:03:37.277084 10182 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I20241027 09:03:37.277119 10182 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I20241027 09:03:37.277130 10182 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "sigmoid1"
  type: "Sigmoid"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I20241027 09:03:37.277271 10182 layer_factory.hpp:77] Creating layer mnist
I20241027 09:03:37.314071 10182 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I20241027 09:03:37.316711 10182 net.cpp:86] Creating Layer mnist
I20241027 09:03:37.316789 10182 net.cpp:382] mnist -> data
I20241027 09:03:37.316879 10182 net.cpp:382] mnist -> label
I20241027 09:03:37.319886 10182 data_layer.cpp:45] output data size: 64,1,28,28
I20241027 09:03:37.321043 10182 net.cpp:124] Setting up mnist
I20241027 09:03:37.321098 10182 net.cpp:131] Top shape: 64 1 28 28 (50176)
I20241027 09:03:37.321141 10182 net.cpp:131] Top shape: 64 (64)
I20241027 09:03:37.321169 10182 net.cpp:139] Memory required for data: 200960
I20241027 09:03:37.321202 10182 layer_factory.hpp:77] Creating layer conv1
I20241027 09:03:37.321265 10182 net.cpp:86] Creating Layer conv1
I20241027 09:03:37.321301 10182 net.cpp:408] conv1 <- data
I20241027 09:03:37.321350 10182 net.cpp:382] conv1 -> conv1
I20241027 09:03:37.322295 10182 net.cpp:124] Setting up conv1
I20241027 09:03:37.322346 10182 net.cpp:131] Top shape: 64 20 24 24 (737280)
I20241027 09:03:37.322396 10182 net.cpp:139] Memory required for data: 3150080
I20241027 09:03:37.322506 10182 layer_factory.hpp:77] Creating layer pool1
I20241027 09:03:37.322547 10182 net.cpp:86] Creating Layer pool1
I20241027 09:03:37.322573 10182 net.cpp:408] pool1 <- conv1
I20241027 09:03:37.322605 10182 net.cpp:382] pool1 -> pool1
I20241027 09:03:37.322717 10182 net.cpp:124] Setting up pool1
I20241027 09:03:37.322746 10182 net.cpp:131] Top shape: 64 20 12 12 (184320)
I20241027 09:03:37.322774 10182 net.cpp:139] Memory required for data: 3887360
I20241027 09:03:37.322798 10182 layer_factory.hpp:77] Creating layer conv2
I20241027 09:03:37.322839 10182 net.cpp:86] Creating Layer conv2
I20241027 09:03:37.322863 10182 net.cpp:408] conv2 <- pool1
I20241027 09:03:37.322896 10182 net.cpp:382] conv2 -> conv2
I20241027 09:03:37.324183 10182 net.cpp:124] Setting up conv2
I20241027 09:03:37.324218 10182 net.cpp:131] Top shape: 64 50 8 8 (204800)
I20241027 09:03:37.324247 10182 net.cpp:139] Memory required for data: 4706560
I20241027 09:03:37.324280 10182 layer_factory.hpp:77] Creating layer pool2
I20241027 09:03:37.324317 10182 net.cpp:86] Creating Layer pool2
I20241027 09:03:37.324343 10182 net.cpp:408] pool2 <- conv2
I20241027 09:03:37.324373 10182 net.cpp:382] pool2 -> pool2
I20241027 09:03:37.324453 10182 net.cpp:124] Setting up pool2
I20241027 09:03:37.324481 10182 net.cpp:131] Top shape: 64 50 4 4 (51200)
I20241027 09:03:37.324518 10182 net.cpp:139] Memory required for data: 4911360
I20241027 09:03:37.324543 10182 layer_factory.hpp:77] Creating layer ip1
I20241027 09:03:37.324579 10182 net.cpp:86] Creating Layer ip1
I20241027 09:03:37.324605 10182 net.cpp:408] ip1 <- pool2
I20241027 09:03:37.324636 10182 net.cpp:382] ip1 -> ip1
I20241027 09:03:37.335366 10182 net.cpp:124] Setting up ip1
I20241027 09:03:37.335405 10182 net.cpp:131] Top shape: 64 500 (32000)
I20241027 09:03:37.335420 10182 net.cpp:139] Memory required for data: 5039360
I20241027 09:03:37.335440 10182 layer_factory.hpp:77] Creating layer sigmoid1
I20241027 09:03:37.335458 10182 net.cpp:86] Creating Layer sigmoid1
I20241027 09:03:37.335471 10182 net.cpp:408] sigmoid1 <- ip1
I20241027 09:03:37.335491 10182 net.cpp:369] sigmoid1 -> ip1 (in-place)
I20241027 09:03:37.335515 10182 net.cpp:124] Setting up sigmoid1
I20241027 09:03:37.335526 10182 net.cpp:131] Top shape: 64 500 (32000)
I20241027 09:03:37.335539 10182 net.cpp:139] Memory required for data: 5167360
I20241027 09:03:37.335551 10182 layer_factory.hpp:77] Creating layer ip2
I20241027 09:03:37.335566 10182 net.cpp:86] Creating Layer ip2
I20241027 09:03:37.335577 10182 net.cpp:408] ip2 <- ip1
I20241027 09:03:37.335592 10182 net.cpp:382] ip2 -> ip2
I20241027 09:03:37.335774 10182 net.cpp:124] Setting up ip2
I20241027 09:03:37.335788 10182 net.cpp:131] Top shape: 64 10 (640)
I20241027 09:03:37.335799 10182 net.cpp:139] Memory required for data: 5169920
I20241027 09:03:37.335813 10182 layer_factory.hpp:77] Creating layer loss
I20241027 09:03:37.335829 10182 net.cpp:86] Creating Layer loss
I20241027 09:03:37.335841 10182 net.cpp:408] loss <- ip2
I20241027 09:03:37.335853 10182 net.cpp:408] loss <- label
I20241027 09:03:37.335867 10182 net.cpp:382] loss -> loss
I20241027 09:03:37.335891 10182 layer_factory.hpp:77] Creating layer loss
I20241027 09:03:37.336028 10182 net.cpp:124] Setting up loss
I20241027 09:03:37.336043 10182 net.cpp:131] Top shape: (1)
I20241027 09:03:37.336056 10182 net.cpp:134]     with loss weight 1
I20241027 09:03:37.336090 10182 net.cpp:139] Memory required for data: 5169924
I20241027 09:03:37.336102 10182 net.cpp:200] loss needs backward computation.
I20241027 09:03:37.336115 10182 net.cpp:200] ip2 needs backward computation.
I20241027 09:03:37.336128 10182 net.cpp:200] sigmoid1 needs backward computation.
I20241027 09:03:37.336139 10182 net.cpp:200] ip1 needs backward computation.
I20241027 09:03:37.336153 10182 net.cpp:200] pool2 needs backward computation.
I20241027 09:03:37.336163 10182 net.cpp:200] conv2 needs backward computation.
I20241027 09:03:37.336176 10182 net.cpp:200] pool1 needs backward computation.
I20241027 09:03:37.336197 10182 net.cpp:200] conv1 needs backward computation.
I20241027 09:03:37.336236 10182 net.cpp:202] mnist does not need backward computation.
I20241027 09:03:37.336247 10182 net.cpp:244] This network produces output loss
I20241027 09:03:37.336269 10182 net.cpp:257] Network initialization done.
I20241027 09:03:37.337361 10182 solver.cpp:190] Creating test net (#0) specified by net file: examples/mnist/lenet_train_test_sigmoid.prototxt
I20241027 09:03:37.337440 10182 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I20241027 09:03:37.337466 10182 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "sigmoid1"
  type: "Sigmoid"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I20241027 09:03:37.337721 10182 layer_factory.hpp:77] Creating layer mnist
I20241027 09:03:37.355950 10182 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I20241027 09:03:37.357427 10182 net.cpp:86] Creating Layer mnist
I20241027 09:03:37.357475 10182 net.cpp:382] mnist -> data
I20241027 09:03:37.357506 10182 net.cpp:382] mnist -> label
I20241027 09:03:37.357676 10182 data_layer.cpp:45] output data size: 100,1,28,28
I20241027 09:03:37.358896 10182 net.cpp:124] Setting up mnist
I20241027 09:03:37.358951 10182 net.cpp:131] Top shape: 100 1 28 28 (78400)
I20241027 09:03:37.358986 10182 net.cpp:131] Top shape: 100 (100)
I20241027 09:03:37.359030 10182 net.cpp:139] Memory required for data: 314000
I20241027 09:03:37.359056 10182 layer_factory.hpp:77] Creating layer label_mnist_1_split
I20241027 09:03:37.359108 10182 net.cpp:86] Creating Layer label_mnist_1_split
I20241027 09:03:37.359136 10182 net.cpp:408] label_mnist_1_split <- label
I20241027 09:03:37.359167 10182 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_0
I20241027 09:03:37.359208 10182 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_1
I20241027 09:03:37.359624 10182 net.cpp:124] Setting up label_mnist_1_split
I20241027 09:03:37.359663 10182 net.cpp:131] Top shape: 100 (100)
I20241027 09:03:37.359690 10182 net.cpp:131] Top shape: 100 (100)
I20241027 09:03:37.359740 10182 net.cpp:139] Memory required for data: 314800
I20241027 09:03:37.359795 10182 layer_factory.hpp:77] Creating layer conv1
I20241027 09:03:37.359845 10182 net.cpp:86] Creating Layer conv1
I20241027 09:03:37.359869 10182 net.cpp:408] conv1 <- data
I20241027 09:03:37.359897 10182 net.cpp:382] conv1 -> conv1
I20241027 09:03:37.360397 10182 net.cpp:124] Setting up conv1
I20241027 09:03:37.360425 10182 net.cpp:131] Top shape: 100 20 24 24 (1152000)
I20241027 09:03:37.360455 10182 net.cpp:139] Memory required for data: 4922800
I20241027 09:03:37.360491 10182 layer_factory.hpp:77] Creating layer pool1
I20241027 09:03:37.360522 10182 net.cpp:86] Creating Layer pool1
I20241027 09:03:37.360545 10182 net.cpp:408] pool1 <- conv1
I20241027 09:03:37.360572 10182 net.cpp:382] pool1 -> pool1
I20241027 09:03:37.360653 10182 net.cpp:124] Setting up pool1
I20241027 09:03:37.360677 10182 net.cpp:131] Top shape: 100 20 12 12 (288000)
I20241027 09:03:37.360702 10182 net.cpp:139] Memory required for data: 6074800
I20241027 09:03:37.360723 10182 layer_factory.hpp:77] Creating layer conv2
I20241027 09:03:37.360762 10182 net.cpp:86] Creating Layer conv2
I20241027 09:03:37.360787 10182 net.cpp:408] conv2 <- pool1
I20241027 09:03:37.360814 10182 net.cpp:382] conv2 -> conv2
I20241027 09:03:37.361956 10182 net.cpp:124] Setting up conv2
I20241027 09:03:37.361991 10182 net.cpp:131] Top shape: 100 50 8 8 (320000)
I20241027 09:03:37.362038 10182 net.cpp:139] Memory required for data: 7354800
I20241027 09:03:37.362071 10182 layer_factory.hpp:77] Creating layer pool2
I20241027 09:03:37.362110 10182 net.cpp:86] Creating Layer pool2
I20241027 09:03:37.362136 10182 net.cpp:408] pool2 <- conv2
I20241027 09:03:37.362164 10182 net.cpp:382] pool2 -> pool2
I20241027 09:03:37.362254 10182 net.cpp:124] Setting up pool2
I20241027 09:03:37.362285 10182 net.cpp:131] Top shape: 100 50 4 4 (80000)
I20241027 09:03:37.362318 10182 net.cpp:139] Memory required for data: 7674800
I20241027 09:03:37.362344 10182 layer_factory.hpp:77] Creating layer ip1
I20241027 09:03:37.362377 10182 net.cpp:86] Creating Layer ip1
I20241027 09:03:37.362406 10182 net.cpp:408] ip1 <- pool2
I20241027 09:03:37.362439 10182 net.cpp:382] ip1 -> ip1
I20241027 09:03:37.375156 10182 net.cpp:124] Setting up ip1
I20241027 09:03:37.375200 10182 net.cpp:131] Top shape: 100 500 (50000)
I20241027 09:03:37.375213 10182 net.cpp:139] Memory required for data: 7874800
I20241027 09:03:37.375232 10182 layer_factory.hpp:77] Creating layer sigmoid1
I20241027 09:03:37.375253 10182 net.cpp:86] Creating Layer sigmoid1
I20241027 09:03:37.375267 10182 net.cpp:408] sigmoid1 <- ip1
I20241027 09:03:37.375281 10182 net.cpp:369] sigmoid1 -> ip1 (in-place)
I20241027 09:03:37.375299 10182 net.cpp:124] Setting up sigmoid1
I20241027 09:03:37.375311 10182 net.cpp:131] Top shape: 100 500 (50000)
I20241027 09:03:37.375324 10182 net.cpp:139] Memory required for data: 8074800
I20241027 09:03:37.375334 10182 layer_factory.hpp:77] Creating layer ip2
I20241027 09:03:37.375351 10182 net.cpp:86] Creating Layer ip2
I20241027 09:03:37.375362 10182 net.cpp:408] ip2 <- ip1
I20241027 09:03:37.375381 10182 net.cpp:382] ip2 -> ip2
I20241027 09:03:37.375567 10182 net.cpp:124] Setting up ip2
I20241027 09:03:37.375581 10182 net.cpp:131] Top shape: 100 10 (1000)
I20241027 09:03:37.375592 10182 net.cpp:139] Memory required for data: 8078800
I20241027 09:03:37.375607 10182 layer_factory.hpp:77] Creating layer ip2_ip2_0_split
I20241027 09:03:37.375625 10182 net.cpp:86] Creating Layer ip2_ip2_0_split
I20241027 09:03:37.375636 10182 net.cpp:408] ip2_ip2_0_split <- ip2
I20241027 09:03:37.375651 10182 net.cpp:382] ip2_ip2_0_split -> ip2_ip2_0_split_0
I20241027 09:03:37.375667 10182 net.cpp:382] ip2_ip2_0_split -> ip2_ip2_0_split_1
I20241027 09:03:37.375710 10182 net.cpp:124] Setting up ip2_ip2_0_split
I20241027 09:03:37.375722 10182 net.cpp:131] Top shape: 100 10 (1000)
I20241027 09:03:37.375734 10182 net.cpp:131] Top shape: 100 10 (1000)
I20241027 09:03:37.375746 10182 net.cpp:139] Memory required for data: 8086800
I20241027 09:03:37.375766 10182 layer_factory.hpp:77] Creating layer accuracy
I20241027 09:03:37.375805 10182 net.cpp:86] Creating Layer accuracy
I20241027 09:03:37.375818 10182 net.cpp:408] accuracy <- ip2_ip2_0_split_0
I20241027 09:03:37.375831 10182 net.cpp:408] accuracy <- label_mnist_1_split_0
I20241027 09:03:37.375845 10182 net.cpp:382] accuracy -> accuracy
I20241027 09:03:37.375864 10182 net.cpp:124] Setting up accuracy
I20241027 09:03:37.375876 10182 net.cpp:131] Top shape: (1)
I20241027 09:03:37.375888 10182 net.cpp:139] Memory required for data: 8086804
I20241027 09:03:37.375900 10182 layer_factory.hpp:77] Creating layer loss
I20241027 09:03:37.375914 10182 net.cpp:86] Creating Layer loss
I20241027 09:03:37.375926 10182 net.cpp:408] loss <- ip2_ip2_0_split_1
I20241027 09:03:37.375938 10182 net.cpp:408] loss <- label_mnist_1_split_1
I20241027 09:03:37.375957 10182 net.cpp:382] loss -> loss
I20241027 09:03:37.375974 10182 layer_factory.hpp:77] Creating layer loss
I20241027 09:03:37.376088 10182 net.cpp:124] Setting up loss
I20241027 09:03:37.376102 10182 net.cpp:131] Top shape: (1)
I20241027 09:03:37.376114 10182 net.cpp:134]     with loss weight 1
I20241027 09:03:37.376133 10182 net.cpp:139] Memory required for data: 8086808
I20241027 09:03:37.376145 10182 net.cpp:200] loss needs backward computation.
I20241027 09:03:37.376158 10182 net.cpp:202] accuracy does not need backward computation.
I20241027 09:03:37.376170 10182 net.cpp:200] ip2_ip2_0_split needs backward computation.
I20241027 09:03:37.376183 10182 net.cpp:200] ip2 needs backward computation.
I20241027 09:03:37.376194 10182 net.cpp:200] sigmoid1 needs backward computation.
I20241027 09:03:37.376205 10182 net.cpp:200] ip1 needs backward computation.
I20241027 09:03:37.376217 10182 net.cpp:200] pool2 needs backward computation.
I20241027 09:03:37.376230 10182 net.cpp:200] conv2 needs backward computation.
I20241027 09:03:37.376241 10182 net.cpp:200] pool1 needs backward computation.
I20241027 09:03:37.376253 10182 net.cpp:200] conv1 needs backward computation.
I20241027 09:03:37.376266 10182 net.cpp:202] label_mnist_1_split does not need backward computation.
I20241027 09:03:37.376282 10182 net.cpp:202] mnist does not need backward computation.
I20241027 09:03:37.376294 10182 net.cpp:244] This network produces output accuracy
I20241027 09:03:37.376307 10182 net.cpp:244] This network produces output loss
I20241027 09:03:37.376330 10182 net.cpp:257] Network initialization done.
I20241027 09:03:37.376382 10182 solver.cpp:57] Solver scaffolding done.
I20241027 09:03:37.376735 10182 caffe.cpp:239] Starting Optimization
I20241027 09:03:37.376749 10182 solver.cpp:289] Solving LeNet
I20241027 09:03:37.376760 10182 solver.cpp:290] Learning Rate Policy: inv
I20241027 09:03:37.377270 10182 solver.cpp:347] Iteration 0, Testing net (#0)
I20241027 09:03:37.731920 10190 data_layer.cpp:73] Restarting data prefetching from start.
I20241027 09:03:37.745361 10182 solver.cpp:414]     Test net output #0: accuracy = 0.101
I20241027 09:03:37.745400 10182 solver.cpp:414]     Test net output #1: loss = 2.38894 (* 1 = 2.38894 loss)
I20241027 09:03:37.751647 10182 solver.cpp:239] Iteration 0 (1.00987e+26 iter/s, 0.374849s/100 iters), loss = 2.38196
I20241027 09:03:37.751690 10182 solver.cpp:258]     Train net output #0: loss = 2.38196 (* 1 = 2.38196 loss)
I20241027 09:03:37.751730 10182 sgd_solver.cpp:112] Iteration 0, lr = 0.01
I20241027 09:03:38.318078 10182 solver.cpp:239] Iteration 100 (176.559 iter/s, 0.566383s/100 iters), loss = 0.384042
I20241027 09:03:38.318125 10182 solver.cpp:258]     Train net output #0: loss = 0.384042 (* 1 = 0.384042 loss)
I20241027 09:03:38.318133 10182 sgd_solver.cpp:112] Iteration 100, lr = 0.00992565
I20241027 09:03:38.884637 10182 solver.cpp:239] Iteration 200 (176.524 iter/s, 0.566496s/100 iters), loss = 0.200403
I20241027 09:03:38.885291 10182 solver.cpp:258]     Train net output #0: loss = 0.200403 (* 1 = 0.200403 loss)
I20241027 09:03:38.885308 10182 sgd_solver.cpp:112] Iteration 200, lr = 0.00985258
I20241027 09:03:39.450246 10182 solver.cpp:239] Iteration 300 (177.005 iter/s, 0.564954s/100 iters), loss = 0.223084
I20241027 09:03:39.450326 10182 solver.cpp:258]     Train net output #0: loss = 0.223084 (* 1 = 0.223084 loss)
I20241027 09:03:39.450337 10182 sgd_solver.cpp:112] Iteration 300, lr = 0.00978075
I20241027 09:03:40.015236 10182 solver.cpp:239] Iteration 400 (177.021 iter/s, 0.564906s/100 iters), loss = 0.142287
I20241027 09:03:40.015280 10182 solver.cpp:258]     Train net output #0: loss = 0.142287 (* 1 = 0.142287 loss)
I20241027 09:03:40.015290 10182 sgd_solver.cpp:112] Iteration 400, lr = 0.00971013
I20241027 09:03:40.574349 10182 solver.cpp:347] Iteration 500, Testing net (#0)
I20241027 09:03:40.891754 10190 data_layer.cpp:73] Restarting data prefetching from start.
I20241027 09:03:40.902416 10182 solver.cpp:414]     Test net output #0: accuracy = 0.9534
I20241027 09:03:40.902437 10182 solver.cpp:414]     Test net output #1: loss = 0.154076 (* 1 = 0.154076 loss)
I20241027 09:03:40.906785 10182 solver.cpp:239] Iteration 500 (112.17 iter/s, 0.891504s/100 iters), loss = 0.198136
I20241027 09:03:40.906806 10182 solver.cpp:258]     Train net output #0: loss = 0.198137 (* 1 = 0.198137 loss)
I20241027 09:03:40.906812 10182 sgd_solver.cpp:112] Iteration 500, lr = 0.00964069
I20241027 09:03:41.333920 10182 solver.cpp:239] Iteration 600 (234.132 iter/s, 0.427109s/100 iters), loss = 0.166011
I20241027 09:03:41.333947 10182 solver.cpp:258]     Train net output #0: loss = 0.166011 (* 1 = 0.166011 loss)
I20241027 09:03:41.333954 10182 sgd_solver.cpp:112] Iteration 600, lr = 0.0095724
I20241027 09:03:41.756352 10182 solver.cpp:239] Iteration 700 (236.742 iter/s, 0.422401s/100 iters), loss = 0.274551
I20241027 09:03:41.756376 10182 solver.cpp:258]     Train net output #0: loss = 0.274551 (* 1 = 0.274551 loss)
I20241027 09:03:41.756383 10182 sgd_solver.cpp:112] Iteration 700, lr = 0.00950522
I20241027 09:03:42.180825 10182 solver.cpp:239] Iteration 800 (235.601 iter/s, 0.424446s/100 iters), loss = 0.240817
I20241027 09:03:42.180850 10182 solver.cpp:258]     Train net output #0: loss = 0.240818 (* 1 = 0.240818 loss)
I20241027 09:03:42.180855 10182 sgd_solver.cpp:112] Iteration 800, lr = 0.00943913
I20241027 09:03:42.607924 10182 solver.cpp:239] Iteration 900 (234.152 iter/s, 0.427072s/100 iters), loss = 0.23218
I20241027 09:03:42.607949 10182 solver.cpp:258]     Train net output #0: loss = 0.23218 (* 1 = 0.23218 loss)
I20241027 09:03:42.607955 10182 sgd_solver.cpp:112] Iteration 900, lr = 0.00937411
I20241027 09:03:42.748111 10189 data_layer.cpp:73] Restarting data prefetching from start.
I20241027 09:03:43.027443 10182 solver.cpp:347] Iteration 1000, Testing net (#0)
I20241027 09:03:43.300912 10190 data_layer.cpp:73] Restarting data prefetching from start.
I20241027 09:03:43.311545 10182 solver.cpp:414]     Test net output #0: accuracy = 0.971
I20241027 09:03:43.311566 10182 solver.cpp:414]     Test net output #1: loss = 0.0938014 (* 1 = 0.0938014 loss)
I20241027 09:03:43.315841 10182 solver.cpp:239] Iteration 1000 (141.265 iter/s, 0.70789s/100 iters), loss = 0.106653
I20241027 09:03:43.315862 10182 solver.cpp:258]     Train net output #0: loss = 0.106653 (* 1 = 0.106653 loss)
I20241027 09:03:43.315868 10182 sgd_solver.cpp:112] Iteration 1000, lr = 0.00931012
I20241027 09:03:43.738896 10182 solver.cpp:239] Iteration 1100 (236.389 iter/s, 0.423031s/100 iters), loss = 0.0359239
I20241027 09:03:43.738924 10182 solver.cpp:258]     Train net output #0: loss = 0.0359239 (* 1 = 0.0359239 loss)
I20241027 09:03:43.738930 10182 sgd_solver.cpp:112] Iteration 1100, lr = 0.00924715
I20241027 09:03:44.162370 10182 solver.cpp:239] Iteration 1200 (236.159 iter/s, 0.423443s/100 iters), loss = 0.0413507
I20241027 09:03:44.162397 10182 solver.cpp:258]     Train net output #0: loss = 0.0413508 (* 1 = 0.0413508 loss)
I20241027 09:03:44.162403 10182 sgd_solver.cpp:112] Iteration 1200, lr = 0.00918515
I20241027 09:03:44.585007 10182 solver.cpp:239] Iteration 1300 (236.628 iter/s, 0.422605s/100 iters), loss = 0.0254585
I20241027 09:03:44.585033 10182 solver.cpp:258]     Train net output #0: loss = 0.0254585 (* 1 = 0.0254585 loss)
I20241027 09:03:44.585044 10182 sgd_solver.cpp:112] Iteration 1300, lr = 0.00912412
I20241027 09:03:45.009063 10182 solver.cpp:239] Iteration 1400 (235.835 iter/s, 0.424026s/100 iters), loss = 0.0217765
I20241027 09:03:45.009086 10182 solver.cpp:258]     Train net output #0: loss = 0.0217765 (* 1 = 0.0217765 loss)
I20241027 09:03:45.009092 10182 sgd_solver.cpp:112] Iteration 1400, lr = 0.00906403
I20241027 09:03:45.428190 10182 solver.cpp:347] Iteration 1500, Testing net (#0)
I20241027 09:03:45.700702 10190 data_layer.cpp:73] Restarting data prefetching from start.
I20241027 09:03:45.711309 10182 solver.cpp:414]     Test net output #0: accuracy = 0.9783
I20241027 09:03:45.711330 10182 solver.cpp:414]     Test net output #1: loss = 0.0735429 (* 1 = 0.0735429 loss)
I20241027 09:03:45.715602 10182 solver.cpp:239] Iteration 1500 (141.54 iter/s, 0.706514s/100 iters), loss = 0.161061
I20241027 09:03:45.715623 10182 solver.cpp:258]     Train net output #0: loss = 0.161062 (* 1 = 0.161062 loss)
I20241027 09:03:45.715629 10182 sgd_solver.cpp:112] Iteration 1500, lr = 0.00900485
I20241027 09:03:46.141831 10182 solver.cpp:239] Iteration 1600 (234.629 iter/s, 0.426205s/100 iters), loss = 0.170791
I20241027 09:03:46.141857 10182 solver.cpp:258]     Train net output #0: loss = 0.170791 (* 1 = 0.170791 loss)
I20241027 09:03:46.141863 10182 sgd_solver.cpp:112] Iteration 1600, lr = 0.00894657
I20241027 09:03:46.564476 10182 solver.cpp:239] Iteration 1700 (236.626 iter/s, 0.422608s/100 iters), loss = 0.0393759
I20241027 09:03:46.564505 10182 solver.cpp:258]     Train net output #0: loss = 0.0393759 (* 1 = 0.0393759 loss)
I20241027 09:03:46.564512 10182 sgd_solver.cpp:112] Iteration 1700, lr = 0.00888916
I20241027 09:03:46.987241 10182 solver.cpp:239] Iteration 1800 (236.556 iter/s, 0.422733s/100 iters), loss = 0.0450311
I20241027 09:03:46.987265 10182 solver.cpp:258]     Train net output #0: loss = 0.0450311 (* 1 = 0.0450311 loss)
I20241027 09:03:46.987272 10182 sgd_solver.cpp:112] Iteration 1800, lr = 0.0088326
I20241027 09:03:47.283869 10189 data_layer.cpp:73] Restarting data prefetching from start.
I20241027 09:03:47.410061 10182 solver.cpp:239] Iteration 1900 (236.522 iter/s, 0.422793s/100 iters), loss = 0.120049
I20241027 09:03:47.410085 10182 solver.cpp:258]     Train net output #0: loss = 0.120049 (* 1 = 0.120049 loss)
I20241027 09:03:47.410091 10182 sgd_solver.cpp:112] Iteration 1900, lr = 0.00877687
I20241027 09:03:47.835306 10182 solver.cpp:347] Iteration 2000, Testing net (#0)
I20241027 09:03:48.109529 10190 data_layer.cpp:73] Restarting data prefetching from start.
I20241027 09:03:48.120213 10182 solver.cpp:414]     Test net output #0: accuracy = 0.9808
I20241027 09:03:48.120234 10182 solver.cpp:414]     Test net output #1: loss = 0.0651277 (* 1 = 0.0651277 loss)
I20241027 09:03:48.124433 10182 solver.cpp:239] Iteration 2000 (139.988 iter/s, 0.714346s/100 iters), loss = 0.0338465
I20241027 09:03:48.124454 10182 solver.cpp:258]     Train net output #0: loss = 0.0338465 (* 1 = 0.0338465 loss)
I20241027 09:03:48.124460 10182 sgd_solver.cpp:112] Iteration 2000, lr = 0.00872196
I20241027 09:03:48.547271 10182 solver.cpp:239] Iteration 2100 (236.511 iter/s, 0.422814s/100 iters), loss = 0.0200533
I20241027 09:03:48.547297 10182 solver.cpp:258]     Train net output #0: loss = 0.0200533 (* 1 = 0.0200533 loss)
I20241027 09:03:48.547304 10182 sgd_solver.cpp:112] Iteration 2100, lr = 0.00866784
I20241027 09:03:48.998333 10182 solver.cpp:239] Iteration 2200 (221.713 iter/s, 0.451033s/100 iters), loss = 0.0694061
I20241027 09:03:48.998368 10182 solver.cpp:258]     Train net output #0: loss = 0.0694061 (* 1 = 0.0694061 loss)
I20241027 09:03:48.998375 10182 sgd_solver.cpp:112] Iteration 2200, lr = 0.0086145
I20241027 09:03:49.549758 10182 solver.cpp:239] Iteration 2300 (181.361 iter/s, 0.551386s/100 iters), loss = 0.156443
I20241027 09:03:49.549803 10182 solver.cpp:258]     Train net output #0: loss = 0.156443 (* 1 = 0.156443 loss)
I20241027 09:03:49.549811 10182 sgd_solver.cpp:112] Iteration 2300, lr = 0.00856192
I20241027 09:03:50.112954 10182 solver.cpp:239] Iteration 2400 (177.574 iter/s, 0.563146s/100 iters), loss = 0.0214975
I20241027 09:03:50.113036 10182 solver.cpp:258]     Train net output #0: loss = 0.0214975 (* 1 = 0.0214975 loss)
I20241027 09:03:50.113049 10182 sgd_solver.cpp:112] Iteration 2400, lr = 0.00851008
I20241027 09:03:50.670519 10182 solver.cpp:347] Iteration 2500, Testing net (#0)
I20241027 09:03:51.013306 10190 data_layer.cpp:73] Restarting data prefetching from start.
I20241027 09:03:51.025660 10182 solver.cpp:414]     Test net output #0: accuracy = 0.976
I20241027 09:03:51.025691 10182 solver.cpp:414]     Test net output #1: loss = 0.0743171 (* 1 = 0.0743171 loss)
I20241027 09:03:51.030582 10182 solver.cpp:239] Iteration 2500 (108.986 iter/s, 0.917545s/100 iters), loss = 0.0450981
I20241027 09:03:51.030612 10182 solver.cpp:258]     Train net output #0: loss = 0.0450981 (* 1 = 0.0450981 loss)
I20241027 09:03:51.030618 10182 sgd_solver.cpp:112] Iteration 2500, lr = 0.00845897
I20241027 09:03:51.457068 10182 solver.cpp:239] Iteration 2600 (234.492 iter/s, 0.426453s/100 iters), loss = 0.0891859
I20241027 09:03:51.457093 10182 solver.cpp:258]     Train net output #0: loss = 0.089186 (* 1 = 0.089186 loss)
I20241027 09:03:51.457100 10182 sgd_solver.cpp:112] Iteration 2600, lr = 0.00840857
I20241027 09:03:51.885262 10182 solver.cpp:239] Iteration 2700 (233.555 iter/s, 0.428165s/100 iters), loss = 0.128626
I20241027 09:03:51.885285 10182 solver.cpp:258]     Train net output #0: loss = 0.128627 (* 1 = 0.128627 loss)
I20241027 09:03:51.885291 10182 sgd_solver.cpp:112] Iteration 2700, lr = 0.00835886
I20241027 09:03:52.306000 10182 solver.cpp:239] Iteration 2800 (237.693 iter/s, 0.420712s/100 iters), loss = 0.00901604
I20241027 09:03:52.306030 10182 solver.cpp:258]     Train net output #0: loss = 0.0090161 (* 1 = 0.0090161 loss)
I20241027 09:03:52.306036 10182 sgd_solver.cpp:112] Iteration 2800, lr = 0.00830984
I20241027 09:03:52.339735 10189 data_layer.cpp:73] Restarting data prefetching from start.
I20241027 09:03:52.727216 10182 solver.cpp:239] Iteration 2900 (237.432 iter/s, 0.421174s/100 iters), loss = 0.0391353
I20241027 09:03:52.727252 10182 solver.cpp:258]     Train net output #0: loss = 0.0391354 (* 1 = 0.0391354 loss)
I20241027 09:03:52.727258 10182 sgd_solver.cpp:112] Iteration 2900, lr = 0.00826148
I20241027 09:03:53.140486 10182 solver.cpp:347] Iteration 3000, Testing net (#0)
I20241027 09:03:53.411891 10190 data_layer.cpp:73] Restarting data prefetching from start.
I20241027 09:03:53.422485 10182 solver.cpp:414]     Test net output #0: accuracy = 0.9794
I20241027 09:03:53.422505 10182 solver.cpp:414]     Test net output #1: loss = 0.0657211 (* 1 = 0.0657211 loss)
I20241027 09:03:53.426743 10182 solver.cpp:239] Iteration 3000 (142.961 iter/s, 0.69949s/100 iters), loss = 0.0351276
I20241027 09:03:53.426764 10182 solver.cpp:258]     Train net output #0: loss = 0.0351276 (* 1 = 0.0351276 loss)
I20241027 09:03:53.426769 10182 sgd_solver.cpp:112] Iteration 3000, lr = 0.00821377
I20241027 09:03:53.844395 10182 solver.cpp:239] Iteration 3100 (239.447 iter/s, 0.417628s/100 iters), loss = 0.0370832
I20241027 09:03:53.844419 10182 solver.cpp:258]     Train net output #0: loss = 0.0370832 (* 1 = 0.0370832 loss)
I20241027 09:03:53.844424 10182 sgd_solver.cpp:112] Iteration 3100, lr = 0.0081667
I20241027 09:03:54.262300 10182 solver.cpp:239] Iteration 3200 (239.305 iter/s, 0.417877s/100 iters), loss = 0.0354514
I20241027 09:03:54.262322 10182 solver.cpp:258]     Train net output #0: loss = 0.0354514 (* 1 = 0.0354514 loss)
I20241027 09:03:54.262328 10182 sgd_solver.cpp:112] Iteration 3200, lr = 0.00812025
I20241027 09:03:54.687114 10182 solver.cpp:239] Iteration 3300 (235.412 iter/s, 0.424787s/100 iters), loss = 0.0463915
I20241027 09:03:54.687137 10182 solver.cpp:258]     Train net output #0: loss = 0.0463915 (* 1 = 0.0463915 loss)
I20241027 09:03:54.687143 10182 sgd_solver.cpp:112] Iteration 3300, lr = 0.00807442
I20241027 09:03:55.104354 10182 solver.cpp:239] Iteration 3400 (239.685 iter/s, 0.417214s/100 iters), loss = 0.0200957
I20241027 09:03:55.104383 10182 solver.cpp:258]     Train net output #0: loss = 0.0200958 (* 1 = 0.0200958 loss)
I20241027 09:03:55.104405 10182 sgd_solver.cpp:112] Iteration 3400, lr = 0.00802918
I20241027 09:03:55.518224 10182 solver.cpp:347] Iteration 3500, Testing net (#0)
I20241027 09:03:55.792845 10190 data_layer.cpp:73] Restarting data prefetching from start.
I20241027 09:03:55.803426 10182 solver.cpp:414]     Test net output #0: accuracy = 0.9826
I20241027 09:03:55.803447 10182 solver.cpp:414]     Test net output #1: loss = 0.0592519 (* 1 = 0.0592519 loss)
I20241027 09:03:55.807703 10182 solver.cpp:239] Iteration 3500 (142.183 iter/s, 0.703317s/100 iters), loss = 0.0143744
I20241027 09:03:55.807722 10182 solver.cpp:258]     Train net output #0: loss = 0.0143746 (* 1 = 0.0143746 loss)
I20241027 09:03:55.807729 10182 sgd_solver.cpp:112] Iteration 3500, lr = 0.00798454
I20241027 09:03:56.226238 10182 solver.cpp:239] Iteration 3600 (238.942 iter/s, 0.418511s/100 iters), loss = 0.0911028
I20241027 09:03:56.226261 10182 solver.cpp:258]     Train net output #0: loss = 0.0911029 (* 1 = 0.0911029 loss)
I20241027 09:03:56.226267 10182 sgd_solver.cpp:112] Iteration 3600, lr = 0.00794046
I20241027 09:03:56.645192 10182 solver.cpp:239] Iteration 3700 (238.708 iter/s, 0.418923s/100 iters), loss = 0.0684608
I20241027 09:03:56.645222 10182 solver.cpp:258]     Train net output #0: loss = 0.0684609 (* 1 = 0.0684609 loss)
I20241027 09:03:56.645229 10182 sgd_solver.cpp:112] Iteration 3700, lr = 0.00789695
I20241027 09:03:56.834290 10189 data_layer.cpp:73] Restarting data prefetching from start.
I20241027 09:03:57.063551 10182 solver.cpp:239] Iteration 3800 (239.047 iter/s, 0.418327s/100 iters), loss = 0.028295
I20241027 09:03:57.063575 10182 solver.cpp:258]     Train net output #0: loss = 0.0282951 (* 1 = 0.0282951 loss)
I20241027 09:03:57.063582 10182 sgd_solver.cpp:112] Iteration 3800, lr = 0.007854
I20241027 09:03:57.482474 10182 solver.cpp:239] Iteration 3900 (238.723 iter/s, 0.418895s/100 iters), loss = 0.0593261
I20241027 09:03:57.482498 10182 solver.cpp:258]     Train net output #0: loss = 0.0593262 (* 1 = 0.0593262 loss)
I20241027 09:03:57.482506 10182 sgd_solver.cpp:112] Iteration 3900, lr = 0.00781158
I20241027 09:03:57.896799 10182 solver.cpp:347] Iteration 4000, Testing net (#0)
I20241027 09:03:58.185755 10190 data_layer.cpp:73] Restarting data prefetching from start.
I20241027 09:03:58.197624 10182 solver.cpp:414]     Test net output #0: accuracy = 0.986
I20241027 09:03:58.197654 10182 solver.cpp:414]     Test net output #1: loss = 0.0448403 (* 1 = 0.0448403 loss)
I20241027 09:03:58.202486 10182 solver.cpp:239] Iteration 4000 (138.892 iter/s, 0.719983s/100 iters), loss = 0.0577524
I20241027 09:03:58.202515 10182 solver.cpp:258]     Train net output #0: loss = 0.0577525 (* 1 = 0.0577525 loss)
I20241027 09:03:58.202523 10182 sgd_solver.cpp:112] Iteration 4000, lr = 0.00776969
I20241027 09:03:58.759611 10182 solver.cpp:239] Iteration 4100 (179.505 iter/s, 0.557089s/100 iters), loss = 0.0273727
I20241027 09:03:58.759655 10182 solver.cpp:258]     Train net output #0: loss = 0.0273727 (* 1 = 0.0273727 loss)
I20241027 09:03:58.759665 10182 sgd_solver.cpp:112] Iteration 4100, lr = 0.00772833
I20241027 09:03:59.324827 10182 solver.cpp:239] Iteration 4200 (176.939 iter/s, 0.565167s/100 iters), loss = 0.0300711
I20241027 09:03:59.324875 10182 solver.cpp:258]     Train net output #0: loss = 0.0300712 (* 1 = 0.0300712 loss)
I20241027 09:03:59.324884 10182 sgd_solver.cpp:112] Iteration 4200, lr = 0.00768748
I20241027 09:03:59.888517 10182 solver.cpp:239] Iteration 4300 (177.42 iter/s, 0.563635s/100 iters), loss = 0.0653763
I20241027 09:03:59.888561 10182 solver.cpp:258]     Train net output #0: loss = 0.0653764 (* 1 = 0.0653764 loss)
I20241027 09:03:59.888571 10182 sgd_solver.cpp:112] Iteration 4300, lr = 0.00764712
I20241027 09:04:00.453087 10182 solver.cpp:239] Iteration 4400 (177.141 iter/s, 0.564521s/100 iters), loss = 0.0567912
I20241027 09:04:00.453131 10182 solver.cpp:258]     Train net output #0: loss = 0.0567913 (* 1 = 0.0567913 loss)
I20241027 09:04:00.453150 10182 sgd_solver.cpp:112] Iteration 4400, lr = 0.00760726
I20241027 09:04:01.012677 10182 solver.cpp:347] Iteration 4500, Testing net (#0)
I20241027 09:04:01.354796 10190 data_layer.cpp:73] Restarting data prefetching from start.
I20241027 09:04:01.368695 10182 solver.cpp:414]     Test net output #0: accuracy = 0.9831
I20241027 09:04:01.368733 10182 solver.cpp:414]     Test net output #1: loss = 0.0524529 (* 1 = 0.0524529 loss)
I20241027 09:04:01.374311 10182 solver.cpp:239] Iteration 4500 (108.557 iter/s, 0.921176s/100 iters), loss = 0.0343936
I20241027 09:04:01.374348 10182 solver.cpp:258]     Train net output #0: loss = 0.0343937 (* 1 = 0.0343937 loss)
I20241027 09:04:01.374356 10182 sgd_solver.cpp:112] Iteration 4500, lr = 0.00756788
I20241027 09:04:01.943496 10182 solver.cpp:239] Iteration 4600 (175.703 iter/s, 0.569142s/100 iters), loss = 0.0127654
I20241027 09:04:01.943545 10182 solver.cpp:258]     Train net output #0: loss = 0.0127655 (* 1 = 0.0127655 loss)
I20241027 09:04:01.943554 10182 sgd_solver.cpp:112] Iteration 4600, lr = 0.00752897
I20241027 09:04:02.410634 10189 data_layer.cpp:73] Restarting data prefetching from start.
I20241027 09:04:02.506561 10182 solver.cpp:239] Iteration 4700 (177.616 iter/s, 0.563012s/100 iters), loss = 0.0329708
I20241027 09:04:02.506604 10182 solver.cpp:258]     Train net output #0: loss = 0.0329709 (* 1 = 0.0329709 loss)
I20241027 09:04:02.506613 10182 sgd_solver.cpp:112] Iteration 4700, lr = 0.00749052
I20241027 09:04:03.073678 10182 solver.cpp:239] Iteration 4800 (176.345 iter/s, 0.567069s/100 iters), loss = 0.0367498
I20241027 09:04:03.073724 10182 solver.cpp:258]     Train net output #0: loss = 0.0367499 (* 1 = 0.0367499 loss)
I20241027 09:04:03.073733 10182 sgd_solver.cpp:112] Iteration 4800, lr = 0.00745253
I20241027 09:04:03.637868 10182 solver.cpp:239] Iteration 4900 (177.261 iter/s, 0.564139s/100 iters), loss = 0.015303
I20241027 09:04:03.637913 10182 solver.cpp:258]     Train net output #0: loss = 0.0153031 (* 1 = 0.0153031 loss)
I20241027 09:04:03.637923 10182 sgd_solver.cpp:112] Iteration 4900, lr = 0.00741498
I20241027 09:04:04.195016 10182 solver.cpp:464] Snapshotting to binary proto file examples/mnist/lenet_sigmoid_iter_5000.caffemodel
I20241027 09:04:04.247107 10182 sgd_solver.cpp:284] Snapshotting solver state to binary proto file examples/mnist/lenet_sigmoid_iter_5000.solverstate
I20241027 09:04:04.301754 10182 solver.cpp:347] Iteration 5000, Testing net (#0)
I20241027 09:04:04.656152 10190 data_layer.cpp:73] Restarting data prefetching from start.
I20241027 09:04:04.669646 10182 solver.cpp:414]     Test net output #0: accuracy = 0.9868
I20241027 09:04:04.669689 10182 solver.cpp:414]     Test net output #1: loss = 0.0405584 (* 1 = 0.0405584 loss)
I20241027 09:04:04.675498 10182 solver.cpp:239] Iteration 5000 (96.3782 iter/s, 1.03758s/100 iters), loss = 0.0644295
I20241027 09:04:04.675537 10182 solver.cpp:258]     Train net output #0: loss = 0.0644296 (* 1 = 0.0644296 loss)
I20241027 09:04:04.675546 10182 sgd_solver.cpp:112] Iteration 5000, lr = 0.00737788
I20241027 09:04:05.236940 10182 solver.cpp:239] Iteration 5100 (178.127 iter/s, 0.561396s/100 iters), loss = 0.0801962
I20241027 09:04:05.236987 10182 solver.cpp:258]     Train net output #0: loss = 0.0801963 (* 1 = 0.0801963 loss)
I20241027 09:04:05.236996 10182 sgd_solver.cpp:112] Iteration 5100, lr = 0.0073412
I20241027 09:04:05.797890 10182 solver.cpp:239] Iteration 5200 (178.288 iter/s, 0.56089s/100 iters), loss = 0.0264344
I20241027 09:04:05.797937 10182 solver.cpp:258]     Train net output #0: loss = 0.0264345 (* 1 = 0.0264345 loss)
I20241027 09:04:05.797946 10182 sgd_solver.cpp:112] Iteration 5200, lr = 0.00730495
I20241027 09:04:06.362306 10182 solver.cpp:239] Iteration 5300 (177.191 iter/s, 0.564363s/100 iters), loss = 0.0134224
I20241027 09:04:06.362380 10182 solver.cpp:258]     Train net output #0: loss = 0.0134225 (* 1 = 0.0134225 loss)
I20241027 09:04:06.362391 10182 sgd_solver.cpp:112] Iteration 5300, lr = 0.00726911
I20241027 09:04:06.926481 10182 solver.cpp:239] Iteration 5400 (177.275 iter/s, 0.564096s/100 iters), loss = 0.0495277
I20241027 09:04:06.926535 10182 solver.cpp:258]     Train net output #0: loss = 0.0495278 (* 1 = 0.0495278 loss)
I20241027 09:04:06.926545 10182 sgd_solver.cpp:112] Iteration 5400, lr = 0.00723368
I20241027 09:04:07.483198 10182 solver.cpp:347] Iteration 5500, Testing net (#0)
I20241027 09:04:07.827127 10190 data_layer.cpp:73] Restarting data prefetching from start.
I20241027 09:04:07.840516 10182 solver.cpp:414]     Test net output #0: accuracy = 0.9877
I20241027 09:04:07.840555 10182 solver.cpp:414]     Test net output #1: loss = 0.0396414 (* 1 = 0.0396414 loss)
I20241027 09:04:07.846158 10182 solver.cpp:239] Iteration 5500 (108.741 iter/s, 0.919619s/100 iters), loss = 0.026139
I20241027 09:04:07.846195 10182 solver.cpp:258]     Train net output #0: loss = 0.0261391 (* 1 = 0.0261391 loss)
I20241027 09:04:07.846203 10182 sgd_solver.cpp:112] Iteration 5500, lr = 0.00719865
I20241027 09:04:08.408380 10182 solver.cpp:239] Iteration 5600 (177.879 iter/s, 0.562179s/100 iters), loss = 0.00685177
I20241027 09:04:08.408425 10182 solver.cpp:258]     Train net output #0: loss = 0.00685192 (* 1 = 0.00685192 loss)
I20241027 09:04:08.408434 10182 sgd_solver.cpp:112] Iteration 5600, lr = 0.00716402
I20241027 09:04:08.521616 10189 data_layer.cpp:73] Restarting data prefetching from start.
I20241027 09:04:08.971400 10182 solver.cpp:239] Iteration 5700 (177.629 iter/s, 0.56297s/100 iters), loss = 0.0140929
I20241027 09:04:08.971446 10182 solver.cpp:258]     Train net output #0: loss = 0.014093 (* 1 = 0.014093 loss)
I20241027 09:04:08.971454 10182 sgd_solver.cpp:112] Iteration 5700, lr = 0.00712977
I20241027 09:04:09.535609 10182 solver.cpp:239] Iteration 5800 (177.255 iter/s, 0.564158s/100 iters), loss = 0.117974
I20241027 09:04:09.535657 10182 solver.cpp:258]     Train net output #0: loss = 0.117974 (* 1 = 0.117974 loss)
I20241027 09:04:09.535667 10182 sgd_solver.cpp:112] Iteration 5800, lr = 0.0070959
I20241027 09:04:10.098536 10182 solver.cpp:239] Iteration 5900 (177.66 iter/s, 0.562872s/100 iters), loss = 0.0180711
I20241027 09:04:10.098580 10182 solver.cpp:258]     Train net output #0: loss = 0.0180713 (* 1 = 0.0180713 loss)
I20241027 09:04:10.098589 10182 sgd_solver.cpp:112] Iteration 5900, lr = 0.0070624
I20241027 09:04:10.657141 10182 solver.cpp:347] Iteration 6000, Testing net (#0)
I20241027 09:04:11.001775 10190 data_layer.cpp:73] Restarting data prefetching from start.
I20241027 09:04:11.015465 10182 solver.cpp:414]     Test net output #0: accuracy = 0.988
I20241027 09:04:11.015502 10182 solver.cpp:414]     Test net output #1: loss = 0.0373276 (* 1 = 0.0373276 loss)
I20241027 09:04:11.021030 10182 solver.cpp:239] Iteration 6000 (108.408 iter/s, 0.922445s/100 iters), loss = 0.0153676
I20241027 09:04:11.021068 10182 solver.cpp:258]     Train net output #0: loss = 0.0153678 (* 1 = 0.0153678 loss)
I20241027 09:04:11.021077 10182 sgd_solver.cpp:112] Iteration 6000, lr = 0.00702927
I20241027 09:04:11.584623 10182 solver.cpp:239] Iteration 6100 (177.447 iter/s, 0.563549s/100 iters), loss = 0.0171325
I20241027 09:04:11.584673 10182 solver.cpp:258]     Train net output #0: loss = 0.0171326 (* 1 = 0.0171326 loss)
I20241027 09:04:11.584682 10182 sgd_solver.cpp:112] Iteration 6100, lr = 0.0069965
I20241027 09:04:12.149614 10182 solver.cpp:239] Iteration 6200 (177.011 iter/s, 0.564936s/100 iters), loss = 0.0227453
I20241027 09:04:12.149658 10182 solver.cpp:258]     Train net output #0: loss = 0.0227455 (* 1 = 0.0227455 loss)
I20241027 09:04:12.149667 10182 sgd_solver.cpp:112] Iteration 6200, lr = 0.00696408
I20241027 09:04:12.712142 10182 solver.cpp:239] Iteration 6300 (177.785 iter/s, 0.562478s/100 iters), loss = 0.0309516
I20241027 09:04:12.712186 10182 solver.cpp:258]     Train net output #0: loss = 0.0309517 (* 1 = 0.0309517 loss)
I20241027 09:04:12.712226 10182 sgd_solver.cpp:112] Iteration 6300, lr = 0.00693201
I20241027 09:04:13.273639 10182 solver.cpp:239] Iteration 6400 (178.111 iter/s, 0.561447s/100 iters), loss = 0.0342453
I20241027 09:04:13.273694 10182 solver.cpp:258]     Train net output #0: loss = 0.0342454 (* 1 = 0.0342454 loss)
I20241027 09:04:13.273703 10182 sgd_solver.cpp:112] Iteration 6400, lr = 0.00690029
I20241027 09:04:13.830523 10182 solver.cpp:347] Iteration 6500, Testing net (#0)
I20241027 09:04:14.175201 10190 data_layer.cpp:73] Restarting data prefetching from start.
I20241027 09:04:14.188207 10182 solver.cpp:414]     Test net output #0: accuracy = 0.9877
I20241027 09:04:14.188246 10182 solver.cpp:414]     Test net output #1: loss = 0.0373608 (* 1 = 0.0373608 loss)
I20241027 09:04:14.193771 10182 solver.cpp:239] Iteration 6500 (108.687 iter/s, 0.920073s/100 iters), loss = 0.0392408
I20241027 09:04:14.193807 10182 solver.cpp:258]     Train net output #0: loss = 0.0392409 (* 1 = 0.0392409 loss)
I20241027 09:04:14.193815 10182 sgd_solver.cpp:112] Iteration 6500, lr = 0.0068689
I20241027 09:04:14.523324 10189 data_layer.cpp:73] Restarting data prefetching from start.
I20241027 09:04:14.759621 10182 solver.cpp:239] Iteration 6600 (176.739 iter/s, 0.565807s/100 iters), loss = 0.0319521
I20241027 09:04:14.759665 10182 solver.cpp:258]     Train net output #0: loss = 0.0319522 (* 1 = 0.0319522 loss)
I20241027 09:04:14.759675 10182 sgd_solver.cpp:112] Iteration 6600, lr = 0.00683784
I20241027 09:04:15.322917 10182 solver.cpp:239] Iteration 6700 (177.543 iter/s, 0.563245s/100 iters), loss = 0.0408418
I20241027 09:04:15.322960 10182 solver.cpp:258]     Train net output #0: loss = 0.0408419 (* 1 = 0.0408419 loss)
I20241027 09:04:15.322969 10182 sgd_solver.cpp:112] Iteration 6700, lr = 0.00680711
I20241027 09:04:15.890018 10182 solver.cpp:239] Iteration 6800 (176.354 iter/s, 0.56704s/100 iters), loss = 0.0157638
I20241027 09:04:15.890066 10182 solver.cpp:258]     Train net output #0: loss = 0.0157639 (* 1 = 0.0157639 loss)
I20241027 09:04:15.890075 10182 sgd_solver.cpp:112] Iteration 6800, lr = 0.0067767
I20241027 09:04:16.452068 10182 solver.cpp:239] Iteration 6900 (177.937 iter/s, 0.561998s/100 iters), loss = 0.0315452
I20241027 09:04:16.452112 10182 solver.cpp:258]     Train net output #0: loss = 0.0315453 (* 1 = 0.0315453 loss)
I20241027 09:04:16.452121 10182 sgd_solver.cpp:112] Iteration 6900, lr = 0.0067466
I20241027 09:04:17.009572 10182 solver.cpp:347] Iteration 7000, Testing net (#0)
I20241027 09:04:17.350630 10190 data_layer.cpp:73] Restarting data prefetching from start.
I20241027 09:04:17.364163 10182 solver.cpp:414]     Test net output #0: accuracy = 0.9885
I20241027 09:04:17.364200 10182 solver.cpp:414]     Test net output #1: loss = 0.037558 (* 1 = 0.037558 loss)
I20241027 09:04:17.369717 10182 solver.cpp:239] Iteration 7000 (108.98 iter/s, 0.917601s/100 iters), loss = 0.0152959
I20241027 09:04:17.369755 10182 solver.cpp:258]     Train net output #0: loss = 0.0152959 (* 1 = 0.0152959 loss)
I20241027 09:04:17.369762 10182 sgd_solver.cpp:112] Iteration 7000, lr = 0.00671681
I20241027 09:04:17.936636 10182 solver.cpp:239] Iteration 7100 (176.406 iter/s, 0.566875s/100 iters), loss = 0.0515431
I20241027 09:04:17.936681 10182 solver.cpp:258]     Train net output #0: loss = 0.0515432 (* 1 = 0.0515432 loss)
I20241027 09:04:17.936689 10182 sgd_solver.cpp:112] Iteration 7100, lr = 0.00668733
I20241027 09:04:18.499536 10182 solver.cpp:239] Iteration 7200 (177.668 iter/s, 0.562849s/100 iters), loss = 0.0102166
I20241027 09:04:18.499580 10182 solver.cpp:258]     Train net output #0: loss = 0.0102166 (* 1 = 0.0102166 loss)
I20241027 09:04:18.499589 10182 sgd_solver.cpp:112] Iteration 7200, lr = 0.00665815
I20241027 09:04:19.063289 10182 solver.cpp:239] Iteration 7300 (177.399 iter/s, 0.563702s/100 iters), loss = 0.0476818
I20241027 09:04:19.063333 10182 solver.cpp:258]     Train net output #0: loss = 0.0476819 (* 1 = 0.0476819 loss)
I20241027 09:04:19.063342 10182 sgd_solver.cpp:112] Iteration 7300, lr = 0.00662927
I20241027 09:04:19.626197 10182 solver.cpp:239] Iteration 7400 (177.665 iter/s, 0.562858s/100 iters), loss = 0.0392044
I20241027 09:04:19.626960 10182 solver.cpp:258]     Train net output #0: loss = 0.0392045 (* 1 = 0.0392045 loss)
I20241027 09:04:19.626987 10182 sgd_solver.cpp:112] Iteration 7400, lr = 0.00660067
I20241027 09:04:20.164896 10189 data_layer.cpp:73] Restarting data prefetching from start.
I20241027 09:04:20.186909 10182 solver.cpp:347] Iteration 7500, Testing net (#0)
I20241027 09:04:20.531080 10190 data_layer.cpp:73] Restarting data prefetching from start.
I20241027 09:04:20.544337 10182 solver.cpp:414]     Test net output #0: accuracy = 0.9864
I20241027 09:04:20.544376 10182 solver.cpp:414]     Test net output #1: loss = 0.0392821 (* 1 = 0.0392821 loss)
I20241027 09:04:20.549894 10182 solver.cpp:239] Iteration 7500 (108.35 iter/s, 0.922933s/100 iters), loss = 0.0098529
I20241027 09:04:20.549932 10182 solver.cpp:258]     Train net output #0: loss = 0.00985298 (* 1 = 0.00985298 loss)
I20241027 09:04:20.549940 10182 sgd_solver.cpp:112] Iteration 7500, lr = 0.00657236
I20241027 09:04:21.114447 10182 solver.cpp:239] Iteration 7600 (177.145 iter/s, 0.56451s/100 iters), loss = 0.0697836
I20241027 09:04:21.114490 10182 solver.cpp:258]     Train net output #0: loss = 0.0697837 (* 1 = 0.0697837 loss)
I20241027 09:04:21.114499 10182 sgd_solver.cpp:112] Iteration 7600, lr = 0.00654433
I20241027 09:04:21.681152 10182 solver.cpp:239] Iteration 7700 (176.474 iter/s, 0.566655s/100 iters), loss = 0.074128
I20241027 09:04:21.681196 10182 solver.cpp:258]     Train net output #0: loss = 0.0741281 (* 1 = 0.0741281 loss)
I20241027 09:04:21.681205 10182 sgd_solver.cpp:112] Iteration 7700, lr = 0.00651658
I20241027 09:04:22.243395 10182 solver.cpp:239] Iteration 7800 (177.875 iter/s, 0.562192s/100 iters), loss = 0.0399021
I20241027 09:04:22.243438 10182 solver.cpp:258]     Train net output #0: loss = 0.0399021 (* 1 = 0.0399021 loss)
I20241027 09:04:22.243448 10182 sgd_solver.cpp:112] Iteration 7800, lr = 0.00648911
I20241027 09:04:22.809048 10182 solver.cpp:239] Iteration 7900 (176.802 iter/s, 0.565603s/100 iters), loss = 0.0197751
I20241027 09:04:22.809092 10182 solver.cpp:258]     Train net output #0: loss = 0.0197752 (* 1 = 0.0197752 loss)
I20241027 09:04:22.809101 10182 sgd_solver.cpp:112] Iteration 7900, lr = 0.0064619
I20241027 09:04:23.366895 10182 solver.cpp:347] Iteration 8000, Testing net (#0)
I20241027 09:04:23.710160 10190 data_layer.cpp:73] Restarting data prefetching from start.
I20241027 09:04:23.723853 10182 solver.cpp:414]     Test net output #0: accuracy = 0.9879
I20241027 09:04:23.723891 10182 solver.cpp:414]     Test net output #1: loss = 0.0358813 (* 1 = 0.0358813 loss)
I20241027 09:04:23.729413 10182 solver.cpp:239] Iteration 8000 (108.658 iter/s, 0.920316s/100 iters), loss = 0.0173547
I20241027 09:04:23.729449 10182 solver.cpp:258]     Train net output #0: loss = 0.0173548 (* 1 = 0.0173548 loss)
I20241027 09:04:23.729458 10182 sgd_solver.cpp:112] Iteration 8000, lr = 0.00643496
I20241027 09:04:24.293105 10182 solver.cpp:239] Iteration 8100 (177.416 iter/s, 0.563649s/100 iters), loss = 0.0548087
I20241027 09:04:24.293148 10182 solver.cpp:258]     Train net output #0: loss = 0.0548088 (* 1 = 0.0548088 loss)
I20241027 09:04:24.293164 10182 sgd_solver.cpp:112] Iteration 8100, lr = 0.00640827
I20241027 09:04:24.855829 10182 solver.cpp:239] Iteration 8200 (177.723 iter/s, 0.562674s/100 iters), loss = 0.0408543
I20241027 09:04:24.855873 10182 solver.cpp:258]     Train net output #0: loss = 0.0408544 (* 1 = 0.0408544 loss)
I20241027 09:04:24.855882 10182 sgd_solver.cpp:112] Iteration 8200, lr = 0.00638185
I20241027 09:04:25.417378 10182 solver.cpp:239] Iteration 8300 (178.095 iter/s, 0.561498s/100 iters), loss = 0.0765684
I20241027 09:04:25.417423 10182 solver.cpp:258]     Train net output #0: loss = 0.0765685 (* 1 = 0.0765685 loss)
I20241027 09:04:25.417431 10182 sgd_solver.cpp:112] Iteration 8300, lr = 0.00635568
I20241027 09:04:25.881341 10182 solver.cpp:239] Iteration 8400 (215.556 iter/s, 0.463917s/100 iters), loss = 0.0836612
I20241027 09:04:25.881381 10182 solver.cpp:258]     Train net output #0: loss = 0.0836612 (* 1 = 0.0836612 loss)
I20241027 09:04:25.881387 10182 sgd_solver.cpp:112] Iteration 8400, lr = 0.00632975
I20241027 09:04:26.019531 10189 data_layer.cpp:73] Restarting data prefetching from start.
I20241027 09:04:26.298668 10182 solver.cpp:347] Iteration 8500, Testing net (#0)
I20241027 09:04:26.569262 10190 data_layer.cpp:73] Restarting data prefetching from start.
I20241027 09:04:26.580087 10182 solver.cpp:414]     Test net output #0: accuracy = 0.9877
I20241027 09:04:26.580107 10182 solver.cpp:414]     Test net output #1: loss = 0.0372431 (* 1 = 0.0372431 loss)
I20241027 09:04:26.584369 10182 solver.cpp:239] Iteration 8500 (142.25 iter/s, 0.702985s/100 iters), loss = 0.0287238
I20241027 09:04:26.584389 10182 solver.cpp:258]     Train net output #0: loss = 0.0287239 (* 1 = 0.0287239 loss)
I20241027 09:04:26.584395 10182 sgd_solver.cpp:112] Iteration 8500, lr = 0.00630407
I20241027 09:04:27.002441 10182 solver.cpp:239] Iteration 8600 (239.208 iter/s, 0.418047s/100 iters), loss = 0.00327362
I20241027 09:04:27.002465 10182 solver.cpp:258]     Train net output #0: loss = 0.00327371 (* 1 = 0.00327371 loss)
I20241027 09:04:27.002470 10182 sgd_solver.cpp:112] Iteration 8600, lr = 0.00627864
I20241027 09:04:27.420858 10182 solver.cpp:239] Iteration 8700 (239.012 iter/s, 0.418389s/100 iters), loss = 0.00771076
I20241027 09:04:27.420882 10182 solver.cpp:258]     Train net output #0: loss = 0.00771086 (* 1 = 0.00771086 loss)
I20241027 09:04:27.420888 10182 sgd_solver.cpp:112] Iteration 8700, lr = 0.00625344
I20241027 09:04:27.845482 10182 solver.cpp:239] Iteration 8800 (235.517 iter/s, 0.424597s/100 iters), loss = 0.00811502
I20241027 09:04:27.845507 10182 solver.cpp:258]     Train net output #0: loss = 0.00811511 (* 1 = 0.00811511 loss)
I20241027 09:04:27.845513 10182 sgd_solver.cpp:112] Iteration 8800, lr = 0.00622847
I20241027 09:04:28.262857 10182 solver.cpp:239] Iteration 8900 (239.609 iter/s, 0.417347s/100 iters), loss = 0.0044771
I20241027 09:04:28.262884 10182 solver.cpp:258]     Train net output #0: loss = 0.0044772 (* 1 = 0.0044772 loss)
I20241027 09:04:28.262890 10182 sgd_solver.cpp:112] Iteration 8900, lr = 0.00620374
I20241027 09:04:28.677381 10182 solver.cpp:347] Iteration 9000, Testing net (#0)
I20241027 09:04:28.949084 10190 data_layer.cpp:73] Restarting data prefetching from start.
I20241027 09:04:28.959811 10182 solver.cpp:414]     Test net output #0: accuracy = 0.9884
I20241027 09:04:28.959831 10182 solver.cpp:414]     Test net output #1: loss = 0.0338771 (* 1 = 0.0338771 loss)
I20241027 09:04:28.964054 10182 solver.cpp:239] Iteration 9000 (142.619 iter/s, 0.701169s/100 iters), loss = 0.0414392
I20241027 09:04:28.964074 10182 solver.cpp:258]     Train net output #0: loss = 0.0414393 (* 1 = 0.0414393 loss)
I20241027 09:04:28.964080 10182 sgd_solver.cpp:112] Iteration 9000, lr = 0.00617924
I20241027 09:04:29.382902 10182 solver.cpp:239] Iteration 9100 (238.764 iter/s, 0.418823s/100 iters), loss = 0.0443294
I20241027 09:04:29.382926 10182 solver.cpp:258]     Train net output #0: loss = 0.0443295 (* 1 = 0.0443295 loss)
I20241027 09:04:29.382932 10182 sgd_solver.cpp:112] Iteration 9100, lr = 0.00615496
I20241027 09:04:29.804287 10182 solver.cpp:239] Iteration 9200 (237.329 iter/s, 0.421357s/100 iters), loss = 0.0217051
I20241027 09:04:29.804311 10182 solver.cpp:258]     Train net output #0: loss = 0.0217052 (* 1 = 0.0217052 loss)
I20241027 09:04:29.804317 10182 sgd_solver.cpp:112] Iteration 9200, lr = 0.0061309
I20241027 09:04:30.223124 10182 solver.cpp:239] Iteration 9300 (238.773 iter/s, 0.418809s/100 iters), loss = 0.0248826
I20241027 09:04:30.223146 10182 solver.cpp:258]     Train net output #0: loss = 0.0248827 (* 1 = 0.0248827 loss)
I20241027 09:04:30.223152 10182 sgd_solver.cpp:112] Iteration 9300, lr = 0.00610706
I20241027 09:04:30.516544 10189 data_layer.cpp:73] Restarting data prefetching from start.
I20241027 09:04:30.642159 10182 solver.cpp:239] Iteration 9400 (238.659 iter/s, 0.419008s/100 iters), loss = 0.111296
I20241027 09:04:30.642199 10182 solver.cpp:258]     Train net output #0: loss = 0.111296 (* 1 = 0.111296 loss)
I20241027 09:04:30.642205 10182 sgd_solver.cpp:112] Iteration 9400, lr = 0.00608343
I20241027 09:04:31.056478 10182 solver.cpp:347] Iteration 9500, Testing net (#0)
I20241027 09:04:31.331660 10190 data_layer.cpp:73] Restarting data prefetching from start.
I20241027 09:04:31.342221 10182 solver.cpp:414]     Test net output #0: accuracy = 0.9879
I20241027 09:04:31.342240 10182 solver.cpp:414]     Test net output #1: loss = 0.0358525 (* 1 = 0.0358525 loss)
I20241027 09:04:31.346478 10182 solver.cpp:239] Iteration 9500 (141.99 iter/s, 0.704277s/100 iters), loss = 0.0116041
I20241027 09:04:31.346499 10182 solver.cpp:258]     Train net output #0: loss = 0.0116042 (* 1 = 0.0116042 loss)
I20241027 09:04:31.346505 10182 sgd_solver.cpp:112] Iteration 9500, lr = 0.00606002
I20241027 09:04:31.763918 10182 solver.cpp:239] Iteration 9600 (239.57 iter/s, 0.417415s/100 iters), loss = 0.0119004
I20241027 09:04:31.763942 10182 solver.cpp:258]     Train net output #0: loss = 0.0119005 (* 1 = 0.0119005 loss)
I20241027 09:04:31.763948 10182 sgd_solver.cpp:112] Iteration 9600, lr = 0.00603682
I20241027 09:04:32.182030 10182 solver.cpp:239] Iteration 9700 (239.187 iter/s, 0.418083s/100 iters), loss = 0.016706
I20241027 09:04:32.182056 10182 solver.cpp:258]     Train net output #0: loss = 0.0167061 (* 1 = 0.0167061 loss)
I20241027 09:04:32.182063 10182 sgd_solver.cpp:112] Iteration 9700, lr = 0.00601382
I20241027 09:04:32.600441 10182 solver.cpp:239] Iteration 9800 (239.017 iter/s, 0.418381s/100 iters), loss = 0.078816
I20241027 09:04:32.600466 10182 solver.cpp:258]     Train net output #0: loss = 0.078816 (* 1 = 0.078816 loss)
I20241027 09:04:32.600471 10182 sgd_solver.cpp:112] Iteration 9800, lr = 0.00599102
I20241027 09:04:33.019405 10182 solver.cpp:239] Iteration 9900 (238.7 iter/s, 0.418936s/100 iters), loss = 0.0125998
I20241027 09:04:33.019429 10182 solver.cpp:258]     Train net output #0: loss = 0.0125998 (* 1 = 0.0125998 loss)
I20241027 09:04:33.019435 10182 sgd_solver.cpp:112] Iteration 9900, lr = 0.00596843
I20241027 09:04:33.434742 10182 solver.cpp:464] Snapshotting to binary proto file examples/mnist/lenet_sigmoid_iter_10000.caffemodel
I20241027 09:04:33.483942 10182 sgd_solver.cpp:284] Snapshotting solver state to binary proto file examples/mnist/lenet_sigmoid_iter_10000.solverstate
I20241027 09:04:33.539891 10182 solver.cpp:327] Iteration 10000, loss = 0.01471
I20241027 09:04:33.539961 10182 solver.cpp:347] Iteration 10000, Testing net (#0)
I20241027 09:04:33.827028 10190 data_layer.cpp:73] Restarting data prefetching from start.
I20241027 09:04:33.837718 10182 solver.cpp:414]     Test net output #0: accuracy = 0.9887
I20241027 09:04:33.837741 10182 solver.cpp:414]     Test net output #1: loss = 0.0344811 (* 1 = 0.0344811 loss)
I20241027 09:04:33.837746 10182 solver.cpp:332] Optimization Done.
I20241027 09:04:33.837750 10182 caffe.cpp:250] Optimization Done.
I20241027 09:04:34.052243 10197 caffe.cpp:275] Use CPU.
I20241027 09:04:34.913460 10197 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I20241027 09:04:34.913501 10197 net.cpp:53] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "sigmoid1"
  type: "Sigmoid"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I20241027 09:04:34.913636 10197 layer_factory.hpp:77] Creating layer mnist
I20241027 09:04:34.928041 10197 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_test_lmdb
I20241027 09:04:34.928965 10197 net.cpp:86] Creating Layer mnist
I20241027 09:04:34.928980 10197 net.cpp:382] mnist -> data
I20241027 09:04:34.929019 10197 net.cpp:382] mnist -> label
I20241027 09:04:34.929059 10197 data_layer.cpp:45] output data size: 100,1,28,28
I20241027 09:04:34.931377 10197 net.cpp:124] Setting up mnist
I20241027 09:04:34.931399 10197 net.cpp:131] Top shape: 100 1 28 28 (78400)
I20241027 09:04:34.931414 10197 net.cpp:131] Top shape: 100 (100)
I20241027 09:04:34.931424 10197 net.cpp:139] Memory required for data: 314000
I20241027 09:04:34.931437 10197 layer_factory.hpp:77] Creating layer label_mnist_1_split
I20241027 09:04:34.931455 10197 net.cpp:86] Creating Layer label_mnist_1_split
I20241027 09:04:34.931465 10197 net.cpp:408] label_mnist_1_split <- label
I20241027 09:04:34.931483 10197 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_0
I20241027 09:04:34.931499 10197 net.cpp:382] label_mnist_1_split -> label_mnist_1_split_1
I20241027 09:04:34.931519 10197 net.cpp:124] Setting up label_mnist_1_split
I20241027 09:04:34.931528 10197 net.cpp:131] Top shape: 100 (100)
I20241027 09:04:34.931537 10197 net.cpp:131] Top shape: 100 (100)
I20241027 09:04:34.931546 10197 net.cpp:139] Memory required for data: 314800
I20241027 09:04:34.931555 10197 layer_factory.hpp:77] Creating layer conv1
I20241027 09:04:34.931581 10197 net.cpp:86] Creating Layer conv1
I20241027 09:04:34.931589 10197 net.cpp:408] conv1 <- data
I20241027 09:04:34.931602 10197 net.cpp:382] conv1 -> conv1
I20241027 09:04:34.931660 10197 net.cpp:124] Setting up conv1
I20241027 09:04:34.931671 10197 net.cpp:131] Top shape: 100 20 24 24 (1152000)
I20241027 09:04:34.931708 10197 net.cpp:139] Memory required for data: 4922800
I20241027 09:04:34.931725 10197 layer_factory.hpp:77] Creating layer pool1
I20241027 09:04:34.931736 10197 net.cpp:86] Creating Layer pool1
I20241027 09:04:34.931744 10197 net.cpp:408] pool1 <- conv1
I20241027 09:04:34.931754 10197 net.cpp:382] pool1 -> pool1
I20241027 09:04:34.931771 10197 net.cpp:124] Setting up pool1
I20241027 09:04:34.931778 10197 net.cpp:131] Top shape: 100 20 12 12 (288000)
I20241027 09:04:34.931787 10197 net.cpp:139] Memory required for data: 6074800
I20241027 09:04:34.931793 10197 layer_factory.hpp:77] Creating layer conv2
I20241027 09:04:34.931804 10197 net.cpp:86] Creating Layer conv2
I20241027 09:04:34.931813 10197 net.cpp:408] conv2 <- pool1
I20241027 09:04:34.931823 10197 net.cpp:382] conv2 -> conv2
I20241027 09:04:34.932124 10197 net.cpp:124] Setting up conv2
I20241027 09:04:34.932134 10197 net.cpp:131] Top shape: 100 50 8 8 (320000)
I20241027 09:04:34.932142 10197 net.cpp:139] Memory required for data: 7354800
I20241027 09:04:34.932152 10197 layer_factory.hpp:77] Creating layer pool2
I20241027 09:04:34.932161 10197 net.cpp:86] Creating Layer pool2
I20241027 09:04:34.932168 10197 net.cpp:408] pool2 <- conv2
I20241027 09:04:34.932181 10197 net.cpp:382] pool2 -> pool2
I20241027 09:04:34.932193 10197 net.cpp:124] Setting up pool2
I20241027 09:04:34.932199 10197 net.cpp:131] Top shape: 100 50 4 4 (80000)
I20241027 09:04:34.932207 10197 net.cpp:139] Memory required for data: 7674800
I20241027 09:04:34.932214 10197 layer_factory.hpp:77] Creating layer ip1
I20241027 09:04:34.932233 10197 net.cpp:86] Creating Layer ip1
I20241027 09:04:34.932240 10197 net.cpp:408] ip1 <- pool2
I20241027 09:04:34.932250 10197 net.cpp:382] ip1 -> ip1
I20241027 09:04:34.938066 10197 net.cpp:124] Setting up ip1
I20241027 09:04:34.938086 10197 net.cpp:131] Top shape: 100 500 (50000)
I20241027 09:04:34.938094 10197 net.cpp:139] Memory required for data: 7874800
I20241027 09:04:34.938105 10197 layer_factory.hpp:77] Creating layer sigmoid1
I20241027 09:04:34.938118 10197 net.cpp:86] Creating Layer sigmoid1
I20241027 09:04:34.938127 10197 net.cpp:408] sigmoid1 <- ip1
I20241027 09:04:34.938134 10197 net.cpp:369] sigmoid1 -> ip1 (in-place)
I20241027 09:04:34.938146 10197 net.cpp:124] Setting up sigmoid1
I20241027 09:04:34.938153 10197 net.cpp:131] Top shape: 100 500 (50000)
I20241027 09:04:34.938159 10197 net.cpp:139] Memory required for data: 8074800
I20241027 09:04:34.938166 10197 layer_factory.hpp:77] Creating layer ip2
I20241027 09:04:34.938175 10197 net.cpp:86] Creating Layer ip2
I20241027 09:04:34.938182 10197 net.cpp:408] ip2 <- ip1
I20241027 09:04:34.938190 10197 net.cpp:382] ip2 -> ip2
I20241027 09:04:34.938252 10197 net.cpp:124] Setting up ip2
I20241027 09:04:34.938259 10197 net.cpp:131] Top shape: 100 10 (1000)
I20241027 09:04:34.938266 10197 net.cpp:139] Memory required for data: 8078800
I20241027 09:04:34.938274 10197 layer_factory.hpp:77] Creating layer ip2_ip2_0_split
I20241027 09:04:34.938282 10197 net.cpp:86] Creating Layer ip2_ip2_0_split
I20241027 09:04:34.938289 10197 net.cpp:408] ip2_ip2_0_split <- ip2
I20241027 09:04:34.938297 10197 net.cpp:382] ip2_ip2_0_split -> ip2_ip2_0_split_0
I20241027 09:04:34.938306 10197 net.cpp:382] ip2_ip2_0_split -> ip2_ip2_0_split_1
I20241027 09:04:34.938315 10197 net.cpp:124] Setting up ip2_ip2_0_split
I20241027 09:04:34.938323 10197 net.cpp:131] Top shape: 100 10 (1000)
I20241027 09:04:34.938328 10197 net.cpp:131] Top shape: 100 10 (1000)
I20241027 09:04:34.938335 10197 net.cpp:139] Memory required for data: 8086800
I20241027 09:04:34.938341 10197 layer_factory.hpp:77] Creating layer accuracy
I20241027 09:04:34.938356 10197 net.cpp:86] Creating Layer accuracy
I20241027 09:04:34.938364 10197 net.cpp:408] accuracy <- ip2_ip2_0_split_0
I20241027 09:04:34.938370 10197 net.cpp:408] accuracy <- label_mnist_1_split_0
I20241027 09:04:34.938380 10197 net.cpp:382] accuracy -> accuracy
I20241027 09:04:34.938388 10197 net.cpp:124] Setting up accuracy
I20241027 09:04:34.938395 10197 net.cpp:131] Top shape: (1)
I20241027 09:04:34.938414 10197 net.cpp:139] Memory required for data: 8086804
I20241027 09:04:34.938421 10197 layer_factory.hpp:77] Creating layer loss
I20241027 09:04:34.938431 10197 net.cpp:86] Creating Layer loss
I20241027 09:04:34.938437 10197 net.cpp:408] loss <- ip2_ip2_0_split_1
I20241027 09:04:34.938444 10197 net.cpp:408] loss <- label_mnist_1_split_1
I20241027 09:04:34.938452 10197 net.cpp:382] loss -> loss
I20241027 09:04:34.938464 10197 layer_factory.hpp:77] Creating layer loss
I20241027 09:04:34.938490 10197 net.cpp:124] Setting up loss
I20241027 09:04:34.938498 10197 net.cpp:131] Top shape: (1)
I20241027 09:04:34.938504 10197 net.cpp:134]     with loss weight 1
I20241027 09:04:34.938520 10197 net.cpp:139] Memory required for data: 8086808
I20241027 09:04:34.938527 10197 net.cpp:200] loss needs backward computation.
I20241027 09:04:34.938534 10197 net.cpp:202] accuracy does not need backward computation.
I20241027 09:04:34.938544 10197 net.cpp:200] ip2_ip2_0_split needs backward computation.
I20241027 09:04:34.938551 10197 net.cpp:200] ip2 needs backward computation.
I20241027 09:04:34.938557 10197 net.cpp:200] sigmoid1 needs backward computation.
I20241027 09:04:34.938565 10197 net.cpp:200] ip1 needs backward computation.
I20241027 09:04:34.938575 10197 net.cpp:200] pool2 needs backward computation.
I20241027 09:04:34.938581 10197 net.cpp:200] conv2 needs backward computation.
I20241027 09:04:34.938588 10197 net.cpp:200] pool1 needs backward computation.
I20241027 09:04:34.938594 10197 net.cpp:200] conv1 needs backward computation.
I20241027 09:04:34.938602 10197 net.cpp:202] label_mnist_1_split does not need backward computation.
I20241027 09:04:34.938609 10197 net.cpp:202] mnist does not need backward computation.
I20241027 09:04:34.938616 10197 net.cpp:244] This network produces output accuracy
I20241027 09:04:34.938623 10197 net.cpp:244] This network produces output loss
I20241027 09:04:34.938637 10197 net.cpp:257] Network initialization done.
I20241027 09:04:34.942857 10197 caffe.cpp:281] Running for 100 iterations.
I20241027 09:04:34.987421 10197 caffe.cpp:304] Batch 0, accuracy = 1
I20241027 09:04:34.987439 10197 caffe.cpp:304] Batch 0, loss = 0.00974271
I20241027 09:04:35.028666 10197 caffe.cpp:304] Batch 1, accuracy = 1
I20241027 09:04:35.028681 10197 caffe.cpp:304] Batch 1, loss = 0.0174396
I20241027 09:04:35.070163 10197 caffe.cpp:304] Batch 2, accuracy = 0.99
I20241027 09:04:35.070180 10197 caffe.cpp:304] Batch 2, loss = 0.0502259
I20241027 09:04:35.110229 10197 caffe.cpp:304] Batch 3, accuracy = 0.99
I20241027 09:04:35.110242 10197 caffe.cpp:304] Batch 3, loss = 0.0386335
I20241027 09:04:35.150377 10197 caffe.cpp:304] Batch 4, accuracy = 0.98
I20241027 09:04:35.150391 10197 caffe.cpp:304] Batch 4, loss = 0.0493759
I20241027 09:04:35.190567 10197 caffe.cpp:304] Batch 5, accuracy = 0.99
I20241027 09:04:35.190579 10197 caffe.cpp:304] Batch 5, loss = 0.0589328
I20241027 09:04:35.230612 10197 caffe.cpp:304] Batch 6, accuracy = 0.97
I20241027 09:04:35.230624 10197 caffe.cpp:304] Batch 6, loss = 0.0520674
I20241027 09:04:35.270711 10197 caffe.cpp:304] Batch 7, accuracy = 0.98
I20241027 09:04:35.270725 10197 caffe.cpp:304] Batch 7, loss = 0.0336372
I20241027 09:04:35.310879 10197 caffe.cpp:304] Batch 8, accuracy = 1
I20241027 09:04:35.310892 10197 caffe.cpp:304] Batch 8, loss = 0.0133676
I20241027 09:04:35.352202 10197 caffe.cpp:304] Batch 9, accuracy = 0.97
I20241027 09:04:35.352218 10197 caffe.cpp:304] Batch 9, loss = 0.0407745
I20241027 09:04:35.392202 10197 caffe.cpp:304] Batch 10, accuracy = 0.97
I20241027 09:04:35.392215 10197 caffe.cpp:304] Batch 10, loss = 0.0682393
I20241027 09:04:35.432308 10197 caffe.cpp:304] Batch 11, accuracy = 0.98
I20241027 09:04:35.432320 10197 caffe.cpp:304] Batch 11, loss = 0.0472601
I20241027 09:04:35.472397 10197 caffe.cpp:304] Batch 12, accuracy = 0.96
I20241027 09:04:35.472410 10197 caffe.cpp:304] Batch 12, loss = 0.137668
I20241027 09:04:35.512437 10197 caffe.cpp:304] Batch 13, accuracy = 0.98
I20241027 09:04:35.512449 10197 caffe.cpp:304] Batch 13, loss = 0.0622586
I20241027 09:04:35.552955 10197 caffe.cpp:304] Batch 14, accuracy = 0.99
I20241027 09:04:35.552969 10197 caffe.cpp:304] Batch 14, loss = 0.0196584
I20241027 09:04:35.593356 10197 caffe.cpp:304] Batch 15, accuracy = 0.96
I20241027 09:04:35.593372 10197 caffe.cpp:304] Batch 15, loss = 0.0783573
I20241027 09:04:35.634667 10197 caffe.cpp:304] Batch 16, accuracy = 0.98
I20241027 09:04:35.634680 10197 caffe.cpp:304] Batch 16, loss = 0.0366643
I20241027 09:04:35.674763 10197 caffe.cpp:304] Batch 17, accuracy = 0.98
I20241027 09:04:35.674777 10197 caffe.cpp:304] Batch 17, loss = 0.0587093
I20241027 09:04:35.714879 10197 caffe.cpp:304] Batch 18, accuracy = 0.99
I20241027 09:04:35.714891 10197 caffe.cpp:304] Batch 18, loss = 0.0233868
I20241027 09:04:35.754957 10197 caffe.cpp:304] Batch 19, accuracy = 0.98
I20241027 09:04:35.754971 10197 caffe.cpp:304] Batch 19, loss = 0.0576166
I20241027 09:04:35.795068 10197 caffe.cpp:304] Batch 20, accuracy = 0.97
I20241027 09:04:35.795080 10197 caffe.cpp:304] Batch 20, loss = 0.0718816
I20241027 09:04:35.835258 10197 caffe.cpp:304] Batch 21, accuracy = 0.97
I20241027 09:04:35.835270 10197 caffe.cpp:304] Batch 21, loss = 0.081969
I20241027 09:04:35.875391 10197 caffe.cpp:304] Batch 22, accuracy = 0.99
I20241027 09:04:35.875409 10197 caffe.cpp:304] Batch 22, loss = 0.0439509
I20241027 09:04:35.917088 10197 caffe.cpp:304] Batch 23, accuracy = 0.99
I20241027 09:04:35.917101 10197 caffe.cpp:304] Batch 23, loss = 0.0566164
I20241027 09:04:35.958618 10197 caffe.cpp:304] Batch 24, accuracy = 0.98
I20241027 09:04:35.958631 10197 caffe.cpp:304] Batch 24, loss = 0.0330767
I20241027 09:04:35.998765 10197 caffe.cpp:304] Batch 25, accuracy = 0.99
I20241027 09:04:35.998777 10197 caffe.cpp:304] Batch 25, loss = 0.0498048
I20241027 09:04:36.038949 10197 caffe.cpp:304] Batch 26, accuracy = 0.98
I20241027 09:04:36.038962 10197 caffe.cpp:304] Batch 26, loss = 0.0904809
I20241027 09:04:36.079109 10197 caffe.cpp:304] Batch 27, accuracy = 1
I20241027 09:04:36.079123 10197 caffe.cpp:304] Batch 27, loss = 0.0147404
I20241027 09:04:36.119212 10197 caffe.cpp:304] Batch 28, accuracy = 0.99
I20241027 09:04:36.119226 10197 caffe.cpp:304] Batch 28, loss = 0.048014
I20241027 09:04:36.159291 10197 caffe.cpp:304] Batch 29, accuracy = 0.97
I20241027 09:04:36.159304 10197 caffe.cpp:304] Batch 29, loss = 0.0767807
I20241027 09:04:36.199545 10197 caffe.cpp:304] Batch 30, accuracy = 0.98
I20241027 09:04:36.199563 10197 caffe.cpp:304] Batch 30, loss = 0.0485142
I20241027 09:04:36.239650 10197 caffe.cpp:304] Batch 31, accuracy = 1
I20241027 09:04:36.239662 10197 caffe.cpp:304] Batch 31, loss = 0.0132887
I20241027 09:04:36.279780 10197 caffe.cpp:304] Batch 32, accuracy = 0.99
I20241027 09:04:36.279793 10197 caffe.cpp:304] Batch 32, loss = 0.0292934
I20241027 09:04:36.319918 10197 caffe.cpp:304] Batch 33, accuracy = 1
I20241027 09:04:36.319931 10197 caffe.cpp:304] Batch 33, loss = 0.00970463
I20241027 09:04:36.360038 10197 caffe.cpp:304] Batch 34, accuracy = 0.98
I20241027 09:04:36.360052 10197 caffe.cpp:304] Batch 34, loss = 0.0537773
I20241027 09:04:36.400151 10197 caffe.cpp:304] Batch 35, accuracy = 0.95
I20241027 09:04:36.400163 10197 caffe.cpp:304] Batch 35, loss = 0.124202
I20241027 09:04:36.440213 10197 caffe.cpp:304] Batch 36, accuracy = 1
I20241027 09:04:36.440225 10197 caffe.cpp:304] Batch 36, loss = 0.00894919
I20241027 09:04:36.480393 10197 caffe.cpp:304] Batch 37, accuracy = 0.98
I20241027 09:04:36.480407 10197 caffe.cpp:304] Batch 37, loss = 0.0765884
I20241027 09:04:36.520588 10197 caffe.cpp:304] Batch 38, accuracy = 0.99
I20241027 09:04:36.520601 10197 caffe.cpp:304] Batch 38, loss = 0.0479809
I20241027 09:04:36.560901 10197 caffe.cpp:304] Batch 39, accuracy = 0.99
I20241027 09:04:36.560917 10197 caffe.cpp:304] Batch 39, loss = 0.0428308
I20241027 09:04:36.601194 10197 caffe.cpp:304] Batch 40, accuracy = 0.97
I20241027 09:04:36.601209 10197 caffe.cpp:304] Batch 40, loss = 0.0797352
I20241027 09:04:36.641249 10197 caffe.cpp:304] Batch 41, accuracy = 0.98
I20241027 09:04:36.641263 10197 caffe.cpp:304] Batch 41, loss = 0.0562682
I20241027 09:04:36.681389 10197 caffe.cpp:304] Batch 42, accuracy = 0.98
I20241027 09:04:36.681403 10197 caffe.cpp:304] Batch 42, loss = 0.0766082
I20241027 09:04:36.721500 10197 caffe.cpp:304] Batch 43, accuracy = 0.99
I20241027 09:04:36.721513 10197 caffe.cpp:304] Batch 43, loss = 0.0202094
I20241027 09:04:36.761631 10197 caffe.cpp:304] Batch 44, accuracy = 0.98
I20241027 09:04:36.761644 10197 caffe.cpp:304] Batch 44, loss = 0.0342045
I20241027 09:04:36.801663 10197 caffe.cpp:304] Batch 45, accuracy = 0.98
I20241027 09:04:36.801676 10197 caffe.cpp:304] Batch 45, loss = 0.0421152
I20241027 09:04:36.841701 10197 caffe.cpp:304] Batch 46, accuracy = 0.99
I20241027 09:04:36.841712 10197 caffe.cpp:304] Batch 46, loss = 0.0242367
I20241027 09:04:36.883945 10197 caffe.cpp:304] Batch 47, accuracy = 1
I20241027 09:04:36.883960 10197 caffe.cpp:304] Batch 47, loss = 0.0163201
I20241027 09:04:36.924088 10197 caffe.cpp:304] Batch 48, accuracy = 0.96
I20241027 09:04:36.924101 10197 caffe.cpp:304] Batch 48, loss = 0.0786039
I20241027 09:04:36.964184 10197 caffe.cpp:304] Batch 49, accuracy = 1
I20241027 09:04:36.964197 10197 caffe.cpp:304] Batch 49, loss = 0.0241519
I20241027 09:04:37.004266 10197 caffe.cpp:304] Batch 50, accuracy = 1
I20241027 09:04:37.004282 10197 caffe.cpp:304] Batch 50, loss = 0.00370788
I20241027 09:04:37.044381 10197 caffe.cpp:304] Batch 51, accuracy = 1
I20241027 09:04:37.044394 10197 caffe.cpp:304] Batch 51, loss = 0.011572
I20241027 09:04:37.084452 10197 caffe.cpp:304] Batch 52, accuracy = 1
I20241027 09:04:37.084465 10197 caffe.cpp:304] Batch 52, loss = 0.0121303
I20241027 09:04:37.124552 10197 caffe.cpp:304] Batch 53, accuracy = 1
I20241027 09:04:37.124565 10197 caffe.cpp:304] Batch 53, loss = 0.00216755
I20241027 09:04:37.164705 10197 caffe.cpp:304] Batch 54, accuracy = 1
I20241027 09:04:37.164717 10197 caffe.cpp:304] Batch 54, loss = 0.0054327
I20241027 09:04:37.204771 10197 caffe.cpp:304] Batch 55, accuracy = 1
I20241027 09:04:37.204784 10197 caffe.cpp:304] Batch 55, loss = 0.00204335
I20241027 09:04:37.244858 10197 caffe.cpp:304] Batch 56, accuracy = 1
I20241027 09:04:37.244870 10197 caffe.cpp:304] Batch 56, loss = 0.025415
I20241027 09:04:37.285140 10197 caffe.cpp:304] Batch 57, accuracy = 0.99
I20241027 09:04:37.285156 10197 caffe.cpp:304] Batch 57, loss = 0.0276113
I20241027 09:04:37.325348 10197 caffe.cpp:304] Batch 58, accuracy = 0.98
I20241027 09:04:37.325361 10197 caffe.cpp:304] Batch 58, loss = 0.0242374
I20241027 09:04:37.365473 10197 caffe.cpp:304] Batch 59, accuracy = 0.97
I20241027 09:04:37.365486 10197 caffe.cpp:304] Batch 59, loss = 0.0899681
I20241027 09:04:37.405578 10197 caffe.cpp:304] Batch 60, accuracy = 1
I20241027 09:04:37.405591 10197 caffe.cpp:304] Batch 60, loss = 0.0196802
I20241027 09:04:37.445786 10197 caffe.cpp:304] Batch 61, accuracy = 0.99
I20241027 09:04:37.445799 10197 caffe.cpp:304] Batch 61, loss = 0.0227511
I20241027 09:04:37.485966 10197 caffe.cpp:304] Batch 62, accuracy = 1
I20241027 09:04:37.485980 10197 caffe.cpp:304] Batch 62, loss = 0.000415288
I20241027 09:04:37.526114 10197 caffe.cpp:304] Batch 63, accuracy = 1
I20241027 09:04:37.526127 10197 caffe.cpp:304] Batch 63, loss = 0.00226759
I20241027 09:04:37.566339 10197 caffe.cpp:304] Batch 64, accuracy = 1
I20241027 09:04:37.566351 10197 caffe.cpp:304] Batch 64, loss = 0.00232277
I20241027 09:04:37.607024 10197 caffe.cpp:304] Batch 65, accuracy = 0.95
I20241027 09:04:37.607041 10197 caffe.cpp:304] Batch 65, loss = 0.13538
I20241027 09:04:37.647080 10197 caffe.cpp:304] Batch 66, accuracy = 0.98
I20241027 09:04:37.647094 10197 caffe.cpp:304] Batch 66, loss = 0.0614086
I20241027 09:04:37.687152 10197 caffe.cpp:304] Batch 67, accuracy = 0.99
I20241027 09:04:37.687165 10197 caffe.cpp:304] Batch 67, loss = 0.0253868
I20241027 09:04:37.729142 10197 caffe.cpp:304] Batch 68, accuracy = 1
I20241027 09:04:37.729161 10197 caffe.cpp:304] Batch 68, loss = 0.00950837
I20241027 09:04:37.769623 10197 caffe.cpp:304] Batch 69, accuracy = 1
I20241027 09:04:37.769637 10197 caffe.cpp:304] Batch 69, loss = 0.00625632
I20241027 09:04:37.809759 10197 caffe.cpp:304] Batch 70, accuracy = 1
I20241027 09:04:37.809772 10197 caffe.cpp:304] Batch 70, loss = 0.00269241
I20241027 09:04:37.849867 10197 caffe.cpp:304] Batch 71, accuracy = 1
I20241027 09:04:37.849881 10197 caffe.cpp:304] Batch 71, loss = 0.00302334
I20241027 09:04:37.889911 10197 caffe.cpp:304] Batch 72, accuracy = 1
I20241027 09:04:37.889925 10197 caffe.cpp:304] Batch 72, loss = 0.0225445
I20241027 09:04:37.929958 10197 caffe.cpp:304] Batch 73, accuracy = 1
I20241027 09:04:37.929970 10197 caffe.cpp:304] Batch 73, loss = 0.0017709
I20241027 09:04:37.970026 10197 caffe.cpp:304] Batch 74, accuracy = 1
I20241027 09:04:37.970038 10197 caffe.cpp:304] Batch 74, loss = 0.020484
I20241027 09:04:38.010109 10197 caffe.cpp:304] Batch 75, accuracy = 1
I20241027 09:04:38.010123 10197 caffe.cpp:304] Batch 75, loss = 0.00397002
I20241027 09:04:38.050248 10197 caffe.cpp:304] Batch 76, accuracy = 1
I20241027 09:04:38.050261 10197 caffe.cpp:304] Batch 76, loss = 0.00101325
I20241027 09:04:38.090447 10197 caffe.cpp:304] Batch 77, accuracy = 1
I20241027 09:04:38.090461 10197 caffe.cpp:304] Batch 77, loss = 0.00179502
I20241027 09:04:38.130626 10197 caffe.cpp:304] Batch 78, accuracy = 1
I20241027 09:04:38.130643 10197 caffe.cpp:304] Batch 78, loss = 0.00902962
I20241027 09:04:38.170787 10197 caffe.cpp:304] Batch 79, accuracy = 1
I20241027 09:04:38.170800 10197 caffe.cpp:304] Batch 79, loss = 0.00726274
I20241027 09:04:38.210911 10197 caffe.cpp:304] Batch 80, accuracy = 1
I20241027 09:04:38.210923 10197 caffe.cpp:304] Batch 80, loss = 0.013958
I20241027 09:04:38.251098 10197 caffe.cpp:304] Batch 81, accuracy = 1
I20241027 09:04:38.251111 10197 caffe.cpp:304] Batch 81, loss = 0.00705706
I20241027 09:04:38.291208 10197 caffe.cpp:304] Batch 82, accuracy = 1
I20241027 09:04:38.291220 10197 caffe.cpp:304] Batch 82, loss = 0.0132427
I20241027 09:04:38.331346 10197 caffe.cpp:304] Batch 83, accuracy = 1
I20241027 09:04:38.331358 10197 caffe.cpp:304] Batch 83, loss = 0.0242513
I20241027 09:04:38.371477 10197 caffe.cpp:304] Batch 84, accuracy = 0.99
I20241027 09:04:38.371491 10197 caffe.cpp:304] Batch 84, loss = 0.0257834
I20241027 09:04:38.411624 10197 caffe.cpp:304] Batch 85, accuracy = 0.99
I20241027 09:04:38.411636 10197 caffe.cpp:304] Batch 85, loss = 0.0153778
I20241027 09:04:38.451778 10197 caffe.cpp:304] Batch 86, accuracy = 1
I20241027 09:04:38.451792 10197 caffe.cpp:304] Batch 86, loss = 0.00133375
I20241027 09:04:38.491931 10197 caffe.cpp:304] Batch 87, accuracy = 1
I20241027 09:04:38.491945 10197 caffe.cpp:304] Batch 87, loss = 0.000639222
I20241027 09:04:38.532054 10197 caffe.cpp:304] Batch 88, accuracy = 1
I20241027 09:04:38.532066 10197 caffe.cpp:304] Batch 88, loss = 0.000511926
I20241027 09:04:38.572511 10197 caffe.cpp:304] Batch 89, accuracy = 1
I20241027 09:04:38.572528 10197 caffe.cpp:304] Batch 89, loss = 0.000722199
I20241027 09:04:38.612627 10197 caffe.cpp:304] Batch 90, accuracy = 0.96
I20241027 09:04:38.612640 10197 caffe.cpp:304] Batch 90, loss = 0.0963833
I20241027 09:04:38.652823 10197 caffe.cpp:304] Batch 91, accuracy = 1
I20241027 09:04:38.652837 10197 caffe.cpp:304] Batch 91, loss = 0.000899121
I20241027 09:04:38.692930 10197 caffe.cpp:304] Batch 92, accuracy = 1
I20241027 09:04:38.692943 10197 caffe.cpp:304] Batch 92, loss = 0.00368538
I20241027 09:04:38.732969 10197 caffe.cpp:304] Batch 93, accuracy = 1
I20241027 09:04:38.732985 10197 caffe.cpp:304] Batch 93, loss = 0.00499285
I20241027 09:04:38.773061 10197 caffe.cpp:304] Batch 94, accuracy = 1
I20241027 09:04:38.773074 10197 caffe.cpp:304] Batch 94, loss = 0.00370614
I20241027 09:04:38.813143 10197 caffe.cpp:304] Batch 95, accuracy = 1
I20241027 09:04:38.813155 10197 caffe.cpp:304] Batch 95, loss = 0.00476616
I20241027 09:04:38.813632 10204 data_layer.cpp:73] Restarting data prefetching from start.
I20241027 09:04:38.854496 10197 caffe.cpp:304] Batch 96, accuracy = 0.97
I20241027 09:04:38.854509 10197 caffe.cpp:304] Batch 96, loss = 0.0798646
I20241027 09:04:38.894611 10197 caffe.cpp:304] Batch 97, accuracy = 0.96
I20241027 09:04:38.894623 10197 caffe.cpp:304] Batch 97, loss = 0.109523
I20241027 09:04:38.934525 10197 caffe.cpp:304] Batch 98, accuracy = 1
I20241027 09:04:38.934537 10197 caffe.cpp:304] Batch 98, loss = 0.0137383
I20241027 09:04:38.974594 10197 caffe.cpp:304] Batch 99, accuracy = 0.99
I20241027 09:04:38.974607 10197 caffe.cpp:304] Batch 99, loss = 0.035512
I20241027 09:04:38.974611 10197 caffe.cpp:309] Loss: 0.0344811
I20241027 09:04:38.974617 10197 caffe.cpp:321] accuracy = 0.9887
I20241027 09:04:38.974625 10197 caffe.cpp:321] loss = 0.0344811 (* 1 = 0.0344811 loss)
